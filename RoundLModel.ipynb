{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5c0418b",
   "metadata": {},
   "source": [
    "# RoundLModel Net\n",
    "RandLA-Net is a deep learning architecture designed for efficient and accurate semantic segmentation of large-scale 3D point clouds.\n",
    "\n",
    "It improves upon PointNet by:\n",
    "\n",
    "Capturing local structure via k-NN neighborhoods\n",
    "\n",
    "Using attention-based feature aggregation\n",
    "\n",
    "Supporting large-scale scenes with random sampling\n",
    "\n",
    "(Amir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2083287",
   "metadata": {},
   "source": [
    "## Helper Tools\n",
    "The helper_tool.py script provides essential tools for data processing and configuration. It includes dataset-specific parameters (like number of classes or layers), methods for loading and subsampling point clouds, and computing class weights. It also provides visualization tools using Open3D. These functions are used by the training script to prepare data, balance classes, and evaluate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711f82bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:251: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "<>:254: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "<>:257: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "<>:251: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "<>:254: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "<>:257: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cpp_wrappers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m sys.path.append(BASE_DIR)\n\u001b[32m     14\u001b[39m sys.path.append(os.path.join(BASE_DIR, \u001b[33m'\u001b[39m\u001b[33mutils\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcpp_wrappers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcpp_subsampling\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgrid_subsampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcpp_subsampling\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnearest_neighbors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnearest_neighbors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnearest_neighbors\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mConfigSemanticKITTI\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cpp_wrappers'"
     ]
    }
   ],
   "source": [
    "from open3d import linux as open3d\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import colorsys, random, os, sys\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(os.path.join(BASE_DIR, 'utils'))\n",
    "\n",
    "import cpp_wrappers.cpp_subsampling.grid_subsampling as cpp_subsampling\n",
    "import nearest_neighbors.lib.python.nearest_neighbors as nearest_neighbors #These are C++ extensions for fast grid subsampling and k-nearest neighbors\n",
    "\n",
    "\n",
    "class ConfigSemanticKITTI: #These store training hyperparameters for each dataset\n",
    "    k_n = 16  # KNN\n",
    "    num_layers = 4  # Number of layers\n",
    "    num_points = 4096 * 11  # Number of input points\n",
    "    num_classes = 19  # Number of valid classes\n",
    "    sub_grid_size = 0.06  # preprocess_parameter\n",
    "\n",
    "    batch_size = 6  # batch_size during training\n",
    "    val_batch_size = 20  # batch_size during validation and test\n",
    "    train_steps = 500  # Number of steps per epochs\n",
    "    val_steps = 100  # Number of validation steps per epoch\n",
    "\n",
    "    sub_sampling_ratio = [4, 4, 4, 4]  # sampling ratio of random sampling at each layer\n",
    "    d_out = [16, 64, 128, 256]  # feature dimension\n",
    "    num_sub_points = [num_points // 4, num_points // 16, num_points // 64, num_points // 256]\n",
    "\n",
    "    noise_init = 3.5  # noise initial parameter\n",
    "    max_epoch = 100  # maximum epoch during training\n",
    "    learning_rate = 1e-2  # initial learning rate\n",
    "    lr_decays = {i: 0.95 for i in range(0, 500)}  # decay rate of learning rate\n",
    "\n",
    "    train_sum_dir = 'train_log'\n",
    "    saving = True\n",
    "    saving_path = None\n",
    "\n",
    "\n",
    "class ConfigS3DIS:\n",
    "    k_n = 16  # KNN\n",
    "    num_layers = 5  # Number of layers\n",
    "    num_points = 40960  # Number of input points\n",
    "    num_classes = 13  # Number of valid classes\n",
    "    sub_grid_size = 0.04  # preprocess_parameter\n",
    "\n",
    "    batch_size = 6  # batch_size during training\n",
    "    val_batch_size = 20  # batch_size during validation and test\n",
    "    train_steps = 500  # Number of steps per epochs\n",
    "    val_steps = 100  # Number of validation steps per epoch\n",
    "\n",
    "    sub_sampling_ratio = [4, 4, 4, 4, 2]  # sampling ratio of random sampling at each layer\n",
    "    d_out = [16, 64, 128, 256, 512]  # feature dimension\n",
    "\n",
    "    noise_init = 3.5  # noise initial parameter\n",
    "    max_epoch = 100  # maximum epoch during training\n",
    "    learning_rate = 1e-2  # initial learning rate\n",
    "    lr_decays = {i: 0.95 for i in range(0, 500)}  # decay rate of learning rate\n",
    "\n",
    "    train_sum_dir = 'train_log'\n",
    "    saving = True\n",
    "    saving_path = None\n",
    "\n",
    "\n",
    "class ConfigSemantic3D:\n",
    "    k_n = 16  # KNN\n",
    "    num_layers = 5  # Number of layers\n",
    "    num_points = 65536  # Number of input points\n",
    "    num_classes = 8  # Number of valid classes\n",
    "    sub_grid_size = 0.06  # preprocess_parameter\n",
    "\n",
    "    batch_size = 4  # batch_size during training\n",
    "    val_batch_size = 16  # batch_size during validation and test\n",
    "    train_steps = 500  # Number of steps per epochs\n",
    "    val_steps = 100  # Number of validation steps per epoch\n",
    "\n",
    "    sub_sampling_ratio = [4, 4, 4, 4, 2]  # sampling ratio of random sampling at each layer\n",
    "    d_out = [16, 64, 128, 256, 512]  # feature dimension\n",
    "\n",
    "    noise_init = 3.5  # noise initial parameter\n",
    "    max_epoch = 100  # maximum epoch during training\n",
    "    learning_rate = 1e-2  # initial learning rate\n",
    "    lr_decays = {i: 0.95 for i in range(0, 500)}  # decay rate of learning rate\n",
    "\n",
    "    train_sum_dir = 'train_log'\n",
    "    saving = True\n",
    "    saving_path = None\n",
    "\n",
    "    augment_scale_anisotropic = True\n",
    "    augment_symmetries = [True, False, False]\n",
    "    augment_rotation = 'vertical'\n",
    "    augment_scale_min = 0.8\n",
    "    augment_scale_max = 1.2\n",
    "    augment_noise = 0.001\n",
    "    augment_occlusion = 'none'\n",
    "    augment_color = 0.8\n",
    "\n",
    "\n",
    "class DataProcessing: #Contains all preprocessing and loading functions.\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_pc_semantic3d(filename):\n",
    "        pc_pd = pd.read_csv(filename, header=None, delim_whitespace=True, dtype=np.float16)\n",
    "        pc = pc_pd.values\n",
    "        return pc\n",
    "\n",
    "    @staticmethod\n",
    "    def load_label_semantic3d(filename):\n",
    "        label_pd = pd.read_csv(filename, header=None, delim_whitespace=True, dtype=np.uint8)\n",
    "        cloud_labels = label_pd.values\n",
    "        return cloud_labels\n",
    "\n",
    "    @staticmethod\n",
    "    def load_pc_kitti(pc_path):\n",
    "        scan = np.fromfile(pc_path, dtype=np.float32)\n",
    "        scan = scan.reshape((-1, 4))\n",
    "        points = scan[:, 0:3]  # get xyz\n",
    "        return points\n",
    "\n",
    "    @staticmethod\n",
    "    def load_label_kitti(label_path, remap_lut):\n",
    "        label = np.fromfile(label_path, dtype=np.uint32)\n",
    "        label = label.reshape((-1))\n",
    "        sem_label = label & 0xFFFF  # semantic label in lower half\n",
    "        inst_label = label >> 16  # instance id in upper half\n",
    "        assert ((sem_label + (inst_label << 16) == label).all())\n",
    "        sem_label = remap_lut[sem_label]\n",
    "        return sem_label.astype(np.int32)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_file_list(dataset_path, test_scan_num):\n",
    "        seq_list = np.sort(os.listdir(dataset_path))\n",
    "\n",
    "        train_file_list = []\n",
    "        test_file_list = []\n",
    "        val_file_list = []\n",
    "        for seq_id in seq_list:\n",
    "            seq_path = join(dataset_path, seq_id)\n",
    "            pc_path = join(seq_path, 'velodyne')\n",
    "            if seq_id == '08':\n",
    "                val_file_list.append([join(pc_path, f) for f in np.sort(os.listdir(pc_path))])\n",
    "                if seq_id == test_scan_num:\n",
    "                    test_file_list.append([join(pc_path, f) for f in np.sort(os.listdir(pc_path))])\n",
    "            elif int(seq_id) >= 11 and seq_id == test_scan_num:\n",
    "                test_file_list.append([join(pc_path, f) for f in np.sort(os.listdir(pc_path))])\n",
    "            elif seq_id in ['00', '01', '02', '03', '04', '05', '06', '07', '09', '10']:\n",
    "                train_file_list.append([join(pc_path, f) for f in np.sort(os.listdir(pc_path))])\n",
    "\n",
    "        train_file_list = np.concatenate(train_file_list, axis=0)\n",
    "        val_file_list = np.concatenate(val_file_list, axis=0)\n",
    "        test_file_list = np.concatenate(test_file_list, axis=0)\n",
    "        return train_file_list, val_file_list, test_file_list\n",
    "\n",
    "    @staticmethod\n",
    "    def knn_search(support_pts, query_pts, k):\n",
    "        \"\"\"\n",
    "        :param support_pts: points you have, B*N1*3\n",
    "        :param query_pts: points you want to know the neighbour index, B*N2*3\n",
    "        :param k: Number of neighbours in knn search\n",
    "        :return: neighbor_idx: neighboring points indexes, B*N2*k\n",
    "        \"\"\"\n",
    "\n",
    "        neighbor_idx = nearest_neighbors.knn_batch(support_pts, query_pts, k, omp=True)\n",
    "        return neighbor_idx.astype(np.int32)\n",
    "\n",
    "    @staticmethod\n",
    "    def data_aug(xyz, color, labels, idx, num_out):\n",
    "        num_in = len(xyz)\n",
    "        dup = np.random.choice(num_in, num_out - num_in)\n",
    "        xyz_dup = xyz[dup, ...]\n",
    "        xyz_aug = np.concatenate([xyz, xyz_dup], 0)\n",
    "        color_dup = color[dup, ...]\n",
    "        color_aug = np.concatenate([color, color_dup], 0)\n",
    "        idx_dup = list(range(num_in)) + list(dup)\n",
    "        idx_aug = idx[idx_dup]\n",
    "        label_aug = labels[idx_dup]\n",
    "        return xyz_aug, color_aug, idx_aug, label_aug\n",
    "\n",
    "    @staticmethod\n",
    "    def shuffle_idx(x):\n",
    "        # random shuffle the index\n",
    "        idx = np.arange(len(x))\n",
    "        np.random.shuffle(idx)\n",
    "        return x[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def shuffle_list(data_list):\n",
    "        indices = np.arange(np.shape(data_list)[0])\n",
    "        np.random.shuffle(indices)\n",
    "        data_list = data_list[indices]\n",
    "        return data_list\n",
    "\n",
    "    @staticmethod\n",
    "    def grid_sub_sampling(points, features=None, labels=None, grid_size=0.1, verbose=0):\n",
    "        \"\"\"\n",
    "        CPP wrapper for a grid sub_sampling (method = barycenter for points and features\n",
    "        :param points: (N, 3) matrix of input points\n",
    "        :param features: optional (N, d) matrix of features (floating number)\n",
    "        :param labels: optional (N,) matrix of integer labels\n",
    "        :param grid_size: parameter defining the size of grid voxels\n",
    "        :param verbose: 1 to display\n",
    "        :return: sub_sampled points, with features and/or labels depending of the input\n",
    "        \"\"\"\n",
    "\n",
    "        if (features is None) and (labels is None):\n",
    "            return cpp_subsampling.compute(points, sampleDl=grid_size, verbose=verbose)\n",
    "        elif labels is None:\n",
    "            return cpp_subsampling.compute(points, features=features, sampleDl=grid_size, verbose=verbose)\n",
    "        elif features is None:\n",
    "            return cpp_subsampling.compute(points, classes=labels, sampleDl=grid_size, verbose=verbose)\n",
    "        else:\n",
    "            return cpp_subsampling.compute(points, features=features, classes=labels, sampleDl=grid_size,\n",
    "                                           verbose=verbose)\n",
    "\n",
    "    @staticmethod\n",
    "    def IoU_from_confusions(confusions):\n",
    "        \"\"\"\n",
    "        Computes IoU from confusion matrices.\n",
    "        :param confusions: ([..., n_c, n_c] np.int32). Can be any dimension, the confusion matrices should be described by\n",
    "        the last axes. n_c = number of classes\n",
    "        :return: ([..., n_c] np.float32) IoU score\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute TP, FP, FN. This assume that the second to last axis counts the truths (like the first axis of a\n",
    "        # confusion matrix), and that the last axis counts the predictions (like the second axis of a confusion matrix)\n",
    "        TP = np.diagonal(confusions, axis1=-2, axis2=-1)\n",
    "        TP_plus_FN = np.sum(confusions, axis=-1)\n",
    "        TP_plus_FP = np.sum(confusions, axis=-2)\n",
    "\n",
    "        # Compute IoU\n",
    "        IoU = TP / (TP_plus_FP + TP_plus_FN - TP + 1e-6)\n",
    "\n",
    "        # Compute mIoU with only the actual classes\n",
    "        mask = TP_plus_FN < 1e-3\n",
    "        counts = np.sum(1 - mask, axis=-1, keepdims=True)\n",
    "        mIoU = np.sum(IoU, axis=-1, keepdims=True) / (counts + 1e-6)\n",
    "\n",
    "        # If class is absent, place mIoU in place of 0 IoU to get the actual mean later\n",
    "        IoU += mask * mIoU\n",
    "        return IoU\n",
    "\n",
    "    @staticmethod\n",
    "    def get_class_weights(dataset_name):\n",
    "        # pre-calculate the number of points in each category\n",
    "        num_per_class = []\n",
    "        if dataset_name is 'S3DIS':\n",
    "            num_per_class = np.array([3370714, 2856755, 4919229, 318158, 375640, 478001, 974733,\n",
    "                                      650464, 791496, 88727, 1284130, 229758, 2272837], dtype=np.int32)\n",
    "        elif dataset_name is 'Semantic3D':\n",
    "            num_per_class = np.array([5181602, 5012952, 6830086, 1311528, 10476365, 946982, 334860, 269353],\n",
    "                                     dtype=np.int32)\n",
    "        elif dataset_name is 'SemanticKITTI':\n",
    "            num_per_class = np.array([55437630, 320797, 541736, 2578735, 3274484, 552662, 184064, 78858,\n",
    "                                      240942562, 17294618, 170599734, 6369672, 230413074, 101130274, 476491114,\n",
    "                                      9833174, 129609852, 4506626, 1168181])\n",
    "        weight = num_per_class / float(sum(num_per_class))\n",
    "        ce_label_weight = 1 / (weight + 0.02)\n",
    "        return np.expand_dims(ce_label_weight, axis=0)\n",
    "\n",
    "\n",
    "class Plot:#Uses Open3D to display point clouds\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def random_colors(N, bright=True, seed=0):\n",
    "        brightness = 1.0 if bright else 0.7\n",
    "        hsv = [(0.15 + i / float(N), 1, brightness) for i in range(N)]\n",
    "        colors = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv))\n",
    "        random.seed(seed)\n",
    "        random.shuffle(colors)\n",
    "        return colors\n",
    "\n",
    "    @staticmethod\n",
    "    def draw_pc(pc_xyzrgb):\n",
    "        pc = open3d.PointCloud()\n",
    "        pc.points = open3d.Vector3dVector(pc_xyzrgb[:, 0:3])\n",
    "        if pc_xyzrgb.shape[1] == 3:\n",
    "            open3d.draw_geometries([pc])\n",
    "            return 0\n",
    "        if np.max(pc_xyzrgb[:, 3:6]) > 20:  ## 0-255\n",
    "            pc.colors = open3d.Vector3dVector(pc_xyzrgb[:, 3:6] / 255.)\n",
    "        else:\n",
    "            pc.colors = open3d.Vector3dVector(pc_xyzrgb[:, 3:6])\n",
    "        open3d.draw_geometries([pc])\n",
    "        return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def draw_pc_sem_ins(pc_xyz, pc_sem_ins, plot_colors=None):\n",
    "        \"\"\"\n",
    "        pc_xyz: 3D coordinates of point clouds\n",
    "        pc_sem_ins: semantic or instance labels\n",
    "        plot_colors: custom color list\n",
    "        \"\"\"\n",
    "        if plot_colors is not None:\n",
    "            ins_colors = plot_colors\n",
    "        else:\n",
    "            ins_colors = Plot.random_colors(len(np.unique(pc_sem_ins)) + 1, seed=2)\n",
    "\n",
    "        ##############################\n",
    "        sem_ins_labels = np.unique(pc_sem_ins)\n",
    "        sem_ins_bbox = []\n",
    "        Y_colors = np.zeros((pc_sem_ins.shape[0], 3))\n",
    "        for id, semins in enumerate(sem_ins_labels):\n",
    "            valid_ind = np.argwhere(pc_sem_ins == semins)[:, 0]\n",
    "            if semins <= -1:\n",
    "                tp = [0, 0, 0]\n",
    "            else:\n",
    "                if plot_colors is not None:\n",
    "                    tp = ins_colors[semins]\n",
    "                else:\n",
    "                    tp = ins_colors[id]\n",
    "\n",
    "            Y_colors[valid_ind] = tp\n",
    "\n",
    "            ### bbox\n",
    "            valid_xyz = pc_xyz[valid_ind]\n",
    "\n",
    "            xmin = np.min(valid_xyz[:, 0]);\n",
    "            xmax = np.max(valid_xyz[:, 0])\n",
    "            ymin = np.min(valid_xyz[:, 1]);\n",
    "            ymax = np.max(valid_xyz[:, 1])\n",
    "            zmin = np.min(valid_xyz[:, 2]);\n",
    "            zmax = np.max(valid_xyz[:, 2])\n",
    "            sem_ins_bbox.append(\n",
    "                [[xmin, ymin, zmin], [xmax, ymax, zmax], [min(tp[0], 1.), min(tp[1], 1.), min(tp[2], 1.)]])\n",
    "\n",
    "        Y_semins = np.concatenate([pc_xyz[:, 0:3], Y_colors], axis=-1)\n",
    "        Plot.draw_pc(Y_semins)\n",
    "        return Y_semins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e181ed",
   "metadata": {},
   "source": [
    "### RoundlNet\n",
    "The Network class defines the full RandLA-Net architecture. It builds an encoder-decoder neural network using point cloud data and neighborhood indices. The encoder extracts local features with attention, while the decoder reconstructs dense predictions. It includes a custom loss with class weighting, and evaluation based on IoU and accuracy. RandLA-Netâ€™s innovation lies in its lightweight local feature aggregation blocks using soft attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874fd866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amiry\\coding\\Road_point_cloud\\helper_tool.py:249: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "  if dataset_name is 'S3DIS':\n",
      "c:\\Users\\amiry\\coding\\Road_point_cloud\\helper_tool.py:252: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "  elif dataset_name is 'Semantic3D':\n",
      "c:\\Users\\amiry\\coding\\Road_point_cloud\\helper_tool.py:255: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "  elif dataset_name is 'SemanticKITTI':\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'linux' from 'open3d' (c:\\Users\\amiry\\miniconda3\\envs\\sklearn-env\\Lib\\site-packages\\open3d\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m makedirs\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhelper_tool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataProcessing \u001b[38;5;28;01mas\u001b[39;00m DP\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amiry\\coding\\Road_point_cloud\\helper_tool.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopen3d\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m linux \u001b[38;5;28;01mas\u001b[39;00m open3d\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m join\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'linux' from 'open3d' (c:\\Users\\amiry\\miniconda3\\envs\\sklearn-env\\Lib\\site-packages\\open3d\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from os.path import exists, join\n",
    "from os import makedirs\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from helper_tool import DataProcessing as DP\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper_tf_util\n",
    "import time\n",
    "\n",
    "\n",
    "def log_out(out_str, f_out):\n",
    "    f_out.write(out_str + '\\n')\n",
    "    f_out.flush()\n",
    "    print(out_str)\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, dataset, config):\n",
    "        flat_inputs = dataset.flat_inputs\n",
    "        self.config = config\n",
    "        # Path of the result folder\n",
    "        if self.config.saving:\n",
    "            if self.config.saving_path is None:\n",
    "                self.saving_path = time.strftime('results/Log_%Y-%m-%d_%H-%M-%S', time.gmtime())\n",
    "            else:\n",
    "                self.saving_path = self.config.saving_path\n",
    "            makedirs(self.saving_path) if not exists(self.saving_path) else None\n",
    "\n",
    "        with tf.variable_scope('inputs'):\n",
    "            self.inputs = dict()\n",
    "            num_layers = self.config.num_layers\n",
    "            self.inputs['xyz'] = flat_inputs[:num_layers]\n",
    "            self.inputs['neigh_idx'] = flat_inputs[num_layers: 2 * num_layers]\n",
    "            self.inputs['sub_idx'] = flat_inputs[2 * num_layers:3 * num_layers]\n",
    "            self.inputs['interp_idx'] = flat_inputs[3 * num_layers:4 * num_layers]\n",
    "            self.inputs['features'] = flat_inputs[4 * num_layers]\n",
    "            self.inputs['labels'] = flat_inputs[4 * num_layers + 1]\n",
    "            self.inputs['input_inds'] = flat_inputs[4 * num_layers + 2]\n",
    "            self.inputs['cloud_inds'] = flat_inputs[4 * num_layers + 3]\n",
    "\n",
    "            self.labels = self.inputs['labels']\n",
    "            self.is_training = tf.placeholder(tf.bool, shape=())\n",
    "            self.training_step = 1\n",
    "            self.training_epoch = 0\n",
    "            self.correct_prediction = 0\n",
    "            self.accuracy = 0\n",
    "            self.mIou_list = [0]\n",
    "            self.class_weights = DP.get_class_weights(dataset.name)\n",
    "            self.Log_file = open('log_train_' + dataset.name + str(dataset.val_split) + '.txt', 'a')\n",
    "\n",
    "        with tf.variable_scope('layers'):\n",
    "            self.logits = self.inference(self.inputs, self.is_training)\n",
    "\n",
    "        #####################################################################\n",
    "        # Ignore the invalid point (unlabeled) when calculating the loss #\n",
    "        #####################################################################\n",
    "        with tf.variable_scope('loss'):\n",
    "            self.logits = tf.reshape(self.logits, [-1, config.num_classes])\n",
    "            self.labels = tf.reshape(self.labels, [-1])\n",
    "\n",
    "            # Boolean mask of points that should be ignored\n",
    "            ignored_bool = tf.zeros_like(self.labels, dtype=tf.bool)\n",
    "            for ign_label in self.config.ignored_label_inds:\n",
    "                ignored_bool = tf.logical_or(ignored_bool, tf.equal(self.labels, ign_label))\n",
    "\n",
    "            # Collect logits and labels that are not ignored\n",
    "            valid_idx = tf.squeeze(tf.where(tf.logical_not(ignored_bool)))\n",
    "            valid_logits = tf.gather(self.logits, valid_idx, axis=0)\n",
    "            valid_labels_init = tf.gather(self.labels, valid_idx, axis=0)\n",
    "\n",
    "            # Reduce label values in the range of logit shape\n",
    "            reducing_list = tf.range(self.config.num_classes, dtype=tf.int32)\n",
    "            inserted_value = tf.zeros((1,), dtype=tf.int32)\n",
    "            for ign_label in self.config.ignored_label_inds:\n",
    "                reducing_list = tf.concat([reducing_list[:ign_label], inserted_value, reducing_list[ign_label:]], 0)\n",
    "            valid_labels = tf.gather(reducing_list, valid_labels_init)\n",
    "\n",
    "            self.loss = self.get_loss(valid_logits, valid_labels, self.class_weights)\n",
    "\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            self.learning_rate = tf.Variable(config.learning_rate, trainable=False, name='learning_rate')\n",
    "            self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "            self.extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "        with tf.variable_scope('results'):\n",
    "            self.correct_prediction = tf.nn.in_top_k(valid_logits, valid_labels, 1)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "            self.prob_logits = tf.nn.softmax(self.logits)\n",
    "\n",
    "            tf.summary.scalar('learning_rate', self.learning_rate)\n",
    "            tf.summary.scalar('loss', self.loss)\n",
    "            tf.summary.scalar('accuracy', self.accuracy)\n",
    "\n",
    "        my_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        self.saver = tf.train.Saver(my_vars, max_to_keep=100)\n",
    "        c_proto = tf.ConfigProto()\n",
    "        c_proto.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=c_proto)\n",
    "        self.merged = tf.summary.merge_all()\n",
    "        self.train_writer = tf.summary.FileWriter(config.train_sum_dir, self.sess.graph)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def inference(self, inputs, is_training): #Initial MLP on input features\n",
    "\n",
    "        d_out = self.config.d_out\n",
    "        feature = inputs['features']\n",
    "        feature = tf.layers.dense(feature, 8, activation=None, name='fc0')\n",
    "        feature = tf.nn.leaky_relu(tf.layers.batch_normalization(feature, -1, 0.99, 1e-6, training=is_training))\n",
    "        feature = tf.expand_dims(feature, axis=2)\n",
    "\n",
    "        # ###########################Encoder############################ Uses k-NN indices + subsampling to capture large receptive fields\n",
    "        f_encoder_list = []\n",
    "        for i in range(self.config.num_layers):\n",
    "            f_encoder_i = self.dilated_res_block(feature, inputs['xyz'][i], inputs['neigh_idx'][i], d_out[i],\n",
    "                                                 'Encoder_layer_' + str(i), is_training)\n",
    "            f_sampled_i = self.random_sample(f_encoder_i, inputs['sub_idx'][i])\n",
    "            feature = f_sampled_i\n",
    "            if i == 0:\n",
    "                f_encoder_list.append(f_encoder_i)\n",
    "            f_encoder_list.append(f_sampled_i)\n",
    "        # ###########################Encoder############################ Converts features to class logits for segmentation\n",
    "\n",
    "        feature = helper_tf_util.conv2d(f_encoder_list[-1], f_encoder_list[-1].get_shape()[3].value, [1, 1],\n",
    "                                        'decoder_0',\n",
    "                                        [1, 1], 'VALID', True, is_training)\n",
    "\n",
    "        # ###########################Decoder############################\n",
    "        f_decoder_list = []\n",
    "        for j in range(self.config.num_layers):\n",
    "            f_interp_i = self.nearest_interpolation(feature, inputs['interp_idx'][-j - 1])\n",
    "            f_decoder_i = helper_tf_util.conv2d_transpose(tf.concat([f_encoder_list[-j - 2], f_interp_i], axis=3),\n",
    "                                                          f_encoder_list[-j - 2].get_shape()[-1].value, [1, 1],\n",
    "                                                          'Decoder_layer_' + str(j), [1, 1], 'VALID', bn=True,\n",
    "                                                          is_training=is_training)\n",
    "            feature = f_decoder_i\n",
    "            f_decoder_list.append(f_decoder_i)\n",
    "        # ###########################Decoder############################\n",
    "\n",
    "        f_layer_fc1 = helper_tf_util.conv2d(f_decoder_list[-1], 64, [1, 1], 'fc1', [1, 1], 'VALID', True, is_training)\n",
    "        f_layer_fc2 = helper_tf_util.conv2d(f_layer_fc1, 32, [1, 1], 'fc2', [1, 1], 'VALID', True, is_training)\n",
    "        f_layer_drop = helper_tf_util.dropout(f_layer_fc2, keep_prob=0.5, is_training=is_training, scope='dp1')\n",
    "        f_layer_fc3 = helper_tf_util.conv2d(f_layer_drop, self.config.num_classes, [1, 1], 'fc', [1, 1], 'VALID', False,\n",
    "                                            is_training, activation_fn=None)\n",
    "        f_out = tf.squeeze(f_layer_fc3, [2])\n",
    "        return f_out\n",
    "\n",
    "    def train(self, dataset):\n",
    "        log_out('****EPOCH {}****'.format(self.training_epoch), self.Log_file)\n",
    "        self.sess.run(dataset.train_init_op)\n",
    "        while self.training_epoch < self.config.max_epoch:\n",
    "            t_start = time.time()\n",
    "            try:\n",
    "                ops = [self.train_op,\n",
    "                       self.extra_update_ops,\n",
    "                       self.merged,\n",
    "                       self.loss,\n",
    "                       self.logits,\n",
    "                       self.labels,\n",
    "                       self.accuracy]\n",
    "                _, _, summary, l_out, probs, labels, acc = self.sess.run(ops, {self.is_training: True})\n",
    "                self.train_writer.add_summary(summary, self.training_step)\n",
    "                t_end = time.time()\n",
    "                if self.training_step % 50 == 0:\n",
    "                    message = 'Step {:08d} L_out={:5.3f} Acc={:4.2f} ''---{:8.2f} ms/batch'\n",
    "                    log_out(message.format(self.training_step, l_out, acc, 1000 * (t_end - t_start)), self.Log_file)\n",
    "                self.training_step += 1\n",
    "\n",
    "            except tf.errors.OutOfRangeError:\n",
    "\n",
    "                m_iou = self.evaluate(dataset)\n",
    "                if m_iou > np.max(self.mIou_list):\n",
    "                    # Save the best model\n",
    "                    snapshot_directory = join(self.saving_path, 'snapshots')\n",
    "                    makedirs(snapshot_directory) if not exists(snapshot_directory) else None\n",
    "                    self.saver.save(self.sess, snapshot_directory + '/snap', global_step=self.training_step)\n",
    "                self.mIou_list.append(m_iou)\n",
    "                log_out('Best m_IoU is: {:5.3f}'.format(max(self.mIou_list)), self.Log_file)\n",
    "\n",
    "                self.training_epoch += 1\n",
    "                self.sess.run(dataset.train_init_op)\n",
    "                # Update learning rate\n",
    "                op = self.learning_rate.assign(tf.multiply(self.learning_rate,\n",
    "                                                           self.config.lr_decays[self.training_epoch]))\n",
    "                self.sess.run(op)\n",
    "                log_out('****EPOCH {}****'.format(self.training_epoch), self.Log_file)\n",
    "\n",
    "            except tf.errors.InvalidArgumentError as e:\n",
    "\n",
    "                print('Caught a NaN error :')\n",
    "                print(e.error_code)\n",
    "                print(e.message)\n",
    "                print(e.op)\n",
    "                print(e.op.name)\n",
    "                print([t.name for t in e.op.inputs])\n",
    "                print([t.name for t in e.op.outputs])\n",
    "\n",
    "                a = 1 / 0\n",
    "\n",
    "        print('finished')\n",
    "        self.sess.close()\n",
    "\n",
    "    def evaluate(self, dataset):\n",
    "\n",
    "        # Initialise iterator with validation data\n",
    "        self.sess.run(dataset.val_init_op)\n",
    "\n",
    "        gt_classes = [0 for _ in range(self.config.num_classes)]\n",
    "        positive_classes = [0 for _ in range(self.config.num_classes)]\n",
    "        true_positive_classes = [0 for _ in range(self.config.num_classes)]\n",
    "        val_total_correct = 0\n",
    "        val_total_seen = 0\n",
    "\n",
    "        for step_id in range(self.config.val_steps):\n",
    "            if step_id % 50 == 0:\n",
    "                print(str(step_id) + ' / ' + str(self.config.val_steps))\n",
    "            try:\n",
    "                ops = (self.prob_logits, self.labels, self.accuracy)\n",
    "                stacked_prob, labels, acc = self.sess.run(ops, {self.is_training: False})\n",
    "                pred = np.argmax(stacked_prob, 1)\n",
    "                if not self.config.ignored_label_inds:\n",
    "                    pred_valid = pred\n",
    "                    labels_valid = labels\n",
    "                else:\n",
    "                    invalid_idx = np.where(labels == self.config.ignored_label_inds)[0]\n",
    "                    labels_valid = np.delete(labels, invalid_idx)\n",
    "                    labels_valid = labels_valid - 1\n",
    "                    pred_valid = np.delete(pred, invalid_idx)\n",
    "\n",
    "                correct = np.sum(pred_valid == labels_valid)\n",
    "                val_total_correct += correct\n",
    "                val_total_seen += len(labels_valid)\n",
    "\n",
    "                conf_matrix = confusion_matrix(labels_valid, pred_valid, np.arange(0, self.config.num_classes, 1))\n",
    "                gt_classes += np.sum(conf_matrix, axis=1)\n",
    "                positive_classes += np.sum(conf_matrix, axis=0)\n",
    "                true_positive_classes += np.diagonal(conf_matrix)\n",
    "\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "\n",
    "        iou_list = []\n",
    "        for n in range(0, self.config.num_classes, 1):\n",
    "            iou = true_positive_classes[n] / float(gt_classes[n] + positive_classes[n] - true_positive_classes[n])\n",
    "            iou_list.append(iou)\n",
    "        mean_iou = sum(iou_list) / float(self.config.num_classes)\n",
    "\n",
    "        log_out('eval accuracy: {}'.format(val_total_correct / float(val_total_seen)), self.Log_file)\n",
    "        log_out('mean IOU:{}'.format(mean_iou), self.Log_file)\n",
    "\n",
    "        mean_iou = 100 * mean_iou\n",
    "        log_out('Mean IoU = {:.1f}%'.format(mean_iou), self.Log_file)\n",
    "        s = '{:5.2f} | '.format(mean_iou)\n",
    "        for IoU in iou_list:\n",
    "            s += '{:5.2f} '.format(100 * IoU)\n",
    "        log_out('-' * len(s), self.Log_file)\n",
    "        log_out(s, self.Log_file)\n",
    "        log_out('-' * len(s) + '\\n', self.Log_file)\n",
    "        return mean_iou\n",
    "\n",
    "    def get_loss(self, logits, labels, pre_cal_weights):\n",
    "        # calculate the weighted cross entropy according to the inverse frequency\n",
    "        class_weights = tf.convert_to_tensor(pre_cal_weights, dtype=tf.float32)\n",
    "        one_hot_labels = tf.one_hot(labels, depth=self.config.num_classes)\n",
    "        weights = tf.reduce_sum(class_weights * one_hot_labels, axis=1)\n",
    "        unweighted_losses = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_labels)\n",
    "        weighted_losses = unweighted_losses * weights\n",
    "        output_loss = tf.reduce_mean(weighted_losses)\n",
    "        return output_loss\n",
    "\n",
    "    def dilated_res_block(self, feature, xyz, neigh_idx, d_out, name, is_training):\n",
    "        f_pc = helper_tf_util.conv2d(feature, d_out // 2, [1, 1], name + 'mlp1', [1, 1], 'VALID', True, is_training)\n",
    "        f_pc = self.building_block(xyz, f_pc, neigh_idx, d_out, name + 'LFA', is_training)\n",
    "        f_pc = helper_tf_util.conv2d(f_pc, d_out * 2, [1, 1], name + 'mlp2', [1, 1], 'VALID', True, is_training,\n",
    "                                     activation_fn=None)\n",
    "        shortcut = helper_tf_util.conv2d(feature, d_out * 2, [1, 1], name + 'shortcut', [1, 1], 'VALID',\n",
    "                                         activation_fn=None, bn=True, is_training=is_training)\n",
    "        return tf.nn.leaky_relu(f_pc + shortcut)\n",
    "\n",
    "    def building_block(self, xyz, feature, neigh_idx, d_out, name, is_training):\n",
    "        d_in = feature.get_shape()[-1].value\n",
    "        f_xyz = self.relative_pos_encoding(xyz, neigh_idx)\n",
    "        f_xyz = helper_tf_util.conv2d(f_xyz, d_in, [1, 1], name + 'mlp1', [1, 1], 'VALID', True, is_training)\n",
    "        f_neighbours = self.gather_neighbour(tf.squeeze(feature, axis=2), neigh_idx)\n",
    "        f_concat = tf.concat([f_neighbours, f_xyz], axis=-1)\n",
    "        f_pc_agg = self.att_pooling(f_concat, d_out // 2, name + 'att_pooling_1', is_training)\n",
    "\n",
    "        f_xyz = helper_tf_util.conv2d(f_xyz, d_out // 2, [1, 1], name + 'mlp2', [1, 1], 'VALID', True, is_training)\n",
    "        f_neighbours = self.gather_neighbour(tf.squeeze(f_pc_agg, axis=2), neigh_idx)\n",
    "        f_concat = tf.concat([f_neighbours, f_xyz], axis=-1)\n",
    "        f_pc_agg = self.att_pooling(f_concat, d_out, name + 'att_pooling_2', is_training)\n",
    "        return f_pc_agg\n",
    "\n",
    "    def relative_pos_encoding(self, xyz, neigh_idx):\n",
    "        neighbor_xyz = self.gather_neighbour(xyz, neigh_idx)\n",
    "        xyz_tile = tf.tile(tf.expand_dims(xyz, axis=2), [1, 1, tf.shape(neigh_idx)[-1], 1])\n",
    "        relative_xyz = xyz_tile - neighbor_xyz\n",
    "        relative_dis = tf.sqrt(tf.reduce_sum(tf.square(relative_xyz), axis=-1, keepdims=True))\n",
    "        relative_feature = tf.concat([relative_dis, relative_xyz, xyz_tile, neighbor_xyz], axis=-1)\n",
    "        return relative_feature\n",
    "\n",
    "    @staticmethod\n",
    "    def random_sample(feature, pool_idx):\n",
    "        \"\"\"\n",
    "        :param feature: [B, N, d] input features matrix\n",
    "        :param pool_idx: [B, N', max_num] N' < N, N' is the selected position after pooling\n",
    "        :return: pool_features = [B, N', d] pooled features matrix\n",
    "        \"\"\"\n",
    "        feature = tf.squeeze(feature, axis=2)\n",
    "        num_neigh = tf.shape(pool_idx)[-1]\n",
    "        d = feature.get_shape()[-1]\n",
    "        batch_size = tf.shape(pool_idx)[0]\n",
    "        pool_idx = tf.reshape(pool_idx, [batch_size, -1])\n",
    "        pool_features = tf.batch_gather(feature, pool_idx)\n",
    "        pool_features = tf.reshape(pool_features, [batch_size, -1, num_neigh, d])\n",
    "        pool_features = tf.reduce_max(pool_features, axis=2, keepdims=True)\n",
    "        return pool_features\n",
    "\n",
    "    @staticmethod\n",
    "    def nearest_interpolation(feature, interp_idx):\n",
    "        \"\"\"\n",
    "        :param feature: [B, N, d] input features matrix\n",
    "        :param interp_idx: [B, up_num_points, 1] nearest neighbour index\n",
    "        :return: [B, up_num_points, d] interpolated features matrix\n",
    "        \"\"\"\n",
    "        feature = tf.squeeze(feature, axis=2)\n",
    "        batch_size = tf.shape(interp_idx)[0]\n",
    "        up_num_points = tf.shape(interp_idx)[1]\n",
    "        interp_idx = tf.reshape(interp_idx, [batch_size, up_num_points])\n",
    "        interpolated_features = tf.batch_gather(feature, interp_idx)\n",
    "        interpolated_features = tf.expand_dims(interpolated_features, axis=2)\n",
    "        return interpolated_features\n",
    "\n",
    "    @staticmethod\n",
    "    def gather_neighbour(pc, neighbor_idx):\n",
    "        # gather the coordinates or features of neighboring points\n",
    "        batch_size = tf.shape(pc)[0]\n",
    "        num_points = tf.shape(pc)[1]\n",
    "        d = pc.get_shape()[2].value\n",
    "        index_input = tf.reshape(neighbor_idx, shape=[batch_size, -1])\n",
    "        features = tf.batch_gather(pc, index_input)\n",
    "        features = tf.reshape(features, [batch_size, num_points, tf.shape(neighbor_idx)[-1], d])\n",
    "        return features\n",
    "\n",
    "    @staticmethod\n",
    "    def att_pooling(feature_set, d_out, name, is_training):\n",
    "        batch_size = tf.shape(feature_set)[0]\n",
    "        num_points = tf.shape(feature_set)[1]\n",
    "        num_neigh = tf.shape(feature_set)[2]\n",
    "        d = feature_set.get_shape()[3].value\n",
    "        f_reshaped = tf.reshape(feature_set, shape=[-1, num_neigh, d])\n",
    "        att_activation = tf.layers.dense(f_reshaped, d, activation=None, use_bias=False, name=name + 'fc')\n",
    "        att_scores = tf.nn.softmax(att_activation, axis=1)\n",
    "        f_agg = f_reshaped * att_scores\n",
    "        f_agg = tf.reduce_sum(f_agg, axis=1)\n",
    "        f_agg = tf.reshape(f_agg, [batch_size, num_points, 1, d])\n",
    "        f_agg = helper_tf_util.conv2d(f_agg, d_out, [1, 1], name + 'mlp', [1, 1], 'VALID', True, is_training)\n",
    "        return f_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa1b34",
   "metadata": {},
   "source": [
    "# Semantic 3D\n",
    "The main script loads the Semantic3D dataset and initializes the model using the configuration class. Based on the selected mode (train, test, or vis), it either trains RandLA-Net, evaluates it using IoU metrics, or visualizes point clouds with semantic labels. Data is loaded and augmented dynamically using TensorFlow's dataset API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af346b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:358: SyntaxWarning: \"is not\" with 'str' literal. Did you mean \"!=\"?\n",
      "<>:358: SyntaxWarning: \"is not\" with 'str' literal. Did you mean \"!=\"?\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'RandLANet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m join, exists\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mRandLANet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Network\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtester_Semantic3D\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelTester\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhelper_ply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m read_ply\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'RandLANet'"
     ]
    }
   ],
   "source": [
    "from os.path import join, exists\n",
    "from RandLANet import Network\n",
    "from tester_Semantic3D import ModelTester\n",
    "from helper_ply import read_ply\n",
    "from helper_tool import Plot\n",
    "from helper_tool import DataProcessing as DP\n",
    "from helper_tool import ConfigSemantic3D as cfg\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle, argparse, os\n",
    "\n",
    "\n",
    "class Semantic3D:\n",
    "    def __init__(self):\n",
    "        self.name = 'Semantic3D'\n",
    "        self.path = '/data/semantic3d'\n",
    "        self.label_to_names = {0: 'unlabeled',\n",
    "                               1: 'man-made terrain',\n",
    "                               2: 'natural terrain',\n",
    "                               3: 'high vegetation',\n",
    "                               4: 'low vegetation',\n",
    "                               5: 'buildings',\n",
    "                               6: 'hard scape',\n",
    "                               7: 'scanning artefacts',\n",
    "                               8: 'cars'}\n",
    "        self.num_classes = len(self.label_to_names)\n",
    "        self.label_values = np.sort([k for k, v in self.label_to_names.items()])\n",
    "        self.label_to_idx = {l: i for i, l in enumerate(self.label_values)}\n",
    "        self.ignored_labels = np.sort([0])\n",
    "\n",
    "        self.original_folder = join(self.path, 'original_data')\n",
    "        self.full_pc_folder = join(self.path, 'original_ply')\n",
    "        self.sub_pc_folder = join(self.path, 'input_{:.3f}'.format(cfg.sub_grid_size))\n",
    "\n",
    "        # Following KPConv to do the train-validation split\n",
    "        self.all_splits = [0, 1, 4, 5, 3, 4, 3, 0, 1, 2, 3, 4, 2, 0, 5]\n",
    "        self.val_split = 1\n",
    "\n",
    "        # Initial training-validation-testing files\n",
    "        self.train_files = []\n",
    "        self.val_files = []\n",
    "        self.test_files = []\n",
    "        cloud_names = [file_name[:-4] for file_name in os.listdir(self.original_folder) if file_name[-4:] == '.txt']\n",
    "        for pc_name in cloud_names:\n",
    "            if exists(join(self.original_folder, pc_name + '.labels')):\n",
    "                self.train_files.append(join(self.sub_pc_folder, pc_name + '.ply'))\n",
    "            else:\n",
    "                self.test_files.append(join(self.full_pc_folder, pc_name + '.ply'))\n",
    "\n",
    "        self.train_files = np.sort(self.train_files)\n",
    "        self.test_files = np.sort(self.test_files)\n",
    "\n",
    "        for i, file_path in enumerate(self.train_files):\n",
    "            if self.all_splits[i] == self.val_split:\n",
    "                self.val_files.append(file_path)\n",
    "\n",
    "        self.train_files = np.sort([x for x in self.train_files if x not in self.val_files])\n",
    "\n",
    "        # Initiate containers\n",
    "        self.val_proj = []\n",
    "        self.val_labels = []\n",
    "        self.test_proj = []\n",
    "        self.test_labels = []\n",
    "\n",
    "        self.possibility = {}\n",
    "        self.min_possibility = {}\n",
    "        self.class_weight = {}\n",
    "        self.input_trees = {'training': [], 'validation': [], 'test': []}\n",
    "        self.input_colors = {'training': [], 'validation': [], 'test': []}\n",
    "        self.input_labels = {'training': [], 'validation': []}\n",
    "\n",
    "        # Ascii files dict for testing\n",
    "        self.ascii_files = {\n",
    "            'MarketplaceFeldkirch_Station4_rgb_intensity-reduced.ply': 'marketsquarefeldkirch4-reduced.labels',\n",
    "            'sg27_station10_rgb_intensity-reduced.ply': 'sg27_10-reduced.labels',\n",
    "            'sg28_Station2_rgb_intensity-reduced.ply': 'sg28_2-reduced.labels',\n",
    "            'StGallenCathedral_station6_rgb_intensity-reduced.ply': 'stgallencathedral6-reduced.labels',\n",
    "            'birdfountain_station1_xyz_intensity_rgb.ply': 'birdfountain1.labels',\n",
    "            'castleblatten_station1_intensity_rgb.ply': 'castleblatten1.labels',\n",
    "            'castleblatten_station5_xyz_intensity_rgb.ply': 'castleblatten5.labels',\n",
    "            'marketplacefeldkirch_station1_intensity_rgb.ply': 'marketsquarefeldkirch1.labels',\n",
    "            'marketplacefeldkirch_station4_intensity_rgb.ply': 'marketsquarefeldkirch4.labels',\n",
    "            'marketplacefeldkirch_station7_intensity_rgb.ply': 'marketsquarefeldkirch7.labels',\n",
    "            'sg27_station10_intensity_rgb.ply': 'sg27_10.labels',\n",
    "            'sg27_station3_intensity_rgb.ply': 'sg27_3.labels',\n",
    "            'sg27_station6_intensity_rgb.ply': 'sg27_6.labels',\n",
    "            'sg27_station8_intensity_rgb.ply': 'sg27_8.labels',\n",
    "            'sg28_station2_intensity_rgb.ply': 'sg28_2.labels',\n",
    "            'sg28_station5_xyz_intensity_rgb.ply': 'sg28_5.labels',\n",
    "            'stgallencathedral_station1_intensity_rgb.ply': 'stgallencathedral1.labels',\n",
    "            'stgallencathedral_station3_intensity_rgb.ply': 'stgallencathedral3.labels',\n",
    "            'stgallencathedral_station6_intensity_rgb.ply': 'stgallencathedral6.labels'}\n",
    "\n",
    "        self.load_sub_sampled_clouds(cfg.sub_grid_size)\n",
    "\n",
    "    def load_sub_sampled_clouds(self, sub_grid_size):\n",
    "\n",
    "        tree_path = join(self.path, 'input_{:.3f}'.format(sub_grid_size))\n",
    "        files = np.hstack((self.train_files, self.val_files, self.test_files))\n",
    "\n",
    "        for i, file_path in enumerate(files):\n",
    "            cloud_name = file_path.split('/')[-1][:-4]\n",
    "            print('Load_pc_' + str(i) + ': ' + cloud_name)\n",
    "            if file_path in self.val_files:\n",
    "                cloud_split = 'validation'\n",
    "            elif file_path in self.train_files:\n",
    "                cloud_split = 'training'\n",
    "            else:\n",
    "                cloud_split = 'test'\n",
    "\n",
    "            # Name of the input files\n",
    "            kd_tree_file = join(tree_path, '{:s}_KDTree.pkl'.format(cloud_name))\n",
    "            sub_ply_file = join(tree_path, '{:s}.ply'.format(cloud_name))\n",
    "\n",
    "            # read ply with data\n",
    "            data = read_ply(sub_ply_file)\n",
    "            sub_colors = np.vstack((data['red'], data['green'], data['blue'])).T\n",
    "            if cloud_split == 'test':\n",
    "                sub_labels = None\n",
    "            else:\n",
    "                sub_labels = data['class']\n",
    "\n",
    "            # Read pkl with search tree\n",
    "            with open(kd_tree_file, 'rb') as f:\n",
    "                search_tree = pickle.load(f)\n",
    "\n",
    "            self.input_trees[cloud_split] += [search_tree]\n",
    "            self.input_colors[cloud_split] += [sub_colors]\n",
    "            if cloud_split in ['training', 'validation']:\n",
    "                self.input_labels[cloud_split] += [sub_labels]\n",
    "\n",
    "        # Get validation and test re_projection indices\n",
    "        print('\\nPreparing reprojection indices for validation and test')\n",
    "\n",
    "        for i, file_path in enumerate(files):\n",
    "\n",
    "            # get cloud name and split\n",
    "            cloud_name = file_path.split('/')[-1][:-4]\n",
    "\n",
    "            # Validation projection and labels\n",
    "            if file_path in self.val_files:\n",
    "                proj_file = join(tree_path, '{:s}_proj.pkl'.format(cloud_name))\n",
    "                with open(proj_file, 'rb') as f:\n",
    "                    proj_idx, labels = pickle.load(f)\n",
    "                self.val_proj += [proj_idx]\n",
    "                self.val_labels += [labels]\n",
    "\n",
    "            # Test projection\n",
    "            if file_path in self.test_files:\n",
    "                proj_file = join(tree_path, '{:s}_proj.pkl'.format(cloud_name))\n",
    "                with open(proj_file, 'rb') as f:\n",
    "                    proj_idx, labels = pickle.load(f)\n",
    "                self.test_proj += [proj_idx]\n",
    "                self.test_labels += [labels]\n",
    "        print('finished')\n",
    "        return\n",
    "\n",
    "    # Generate the input data flow\n",
    "    def get_batch_gen(self, split):\n",
    "        if split == 'training':\n",
    "            num_per_epoch = cfg.train_steps * cfg.batch_size\n",
    "        elif split == 'validation':\n",
    "            num_per_epoch = cfg.val_steps * cfg.val_batch_size\n",
    "        elif split == 'test':\n",
    "            num_per_epoch = cfg.val_steps * cfg.val_batch_size\n",
    "\n",
    "        # Reset possibility\n",
    "        self.possibility[split] = []\n",
    "        self.min_possibility[split] = []\n",
    "        self.class_weight[split] = []\n",
    "\n",
    "        # Random initialize\n",
    "        for i, tree in enumerate(self.input_trees[split]):\n",
    "            self.possibility[split] += [np.random.rand(tree.data.shape[0]) * 1e-3]\n",
    "            self.min_possibility[split] += [float(np.min(self.possibility[split][-1]))]\n",
    "\n",
    "        if split != 'test':\n",
    "            _, num_class_total = np.unique(np.hstack(self.input_labels[split]), return_counts=True)\n",
    "            self.class_weight[split] += [np.squeeze([num_class_total / np.sum(num_class_total)], axis=0)]\n",
    "\n",
    "        def spatially_regular_gen():\n",
    "\n",
    "            # Generator loop\n",
    "            for i in range(num_per_epoch):  # num_per_epoch\n",
    "\n",
    "                # Choose the cloud with the lowest probability\n",
    "                cloud_idx = int(np.argmin(self.min_possibility[split]))\n",
    "\n",
    "                # choose the point with the minimum of possibility in the cloud as query point\n",
    "                point_ind = np.argmin(self.possibility[split][cloud_idx])\n",
    "\n",
    "                # Get all points within the cloud from tree structure\n",
    "                points = np.array(self.input_trees[split][cloud_idx].data, copy=False)\n",
    "\n",
    "                # Center point of input region\n",
    "                center_point = points[point_ind, :].reshape(1, -1)\n",
    "\n",
    "                # Add noise to the center point\n",
    "                noise = np.random.normal(scale=cfg.noise_init / 10, size=center_point.shape)\n",
    "                pick_point = center_point + noise.astype(center_point.dtype)\n",
    "                query_idx = self.input_trees[split][cloud_idx].query(pick_point, k=cfg.num_points)[1][0]\n",
    "\n",
    "                # Shuffle index\n",
    "                query_idx = DP.shuffle_idx(query_idx)\n",
    "\n",
    "                # Get corresponding points and colors based on the index\n",
    "                queried_pc_xyz = points[query_idx]\n",
    "                queried_pc_xyz[:, 0:2] = queried_pc_xyz[:, 0:2] - pick_point[:, 0:2]\n",
    "                queried_pc_colors = self.input_colors[split][cloud_idx][query_idx]\n",
    "                if split == 'test':\n",
    "                    queried_pc_labels = np.zeros(queried_pc_xyz.shape[0])\n",
    "                    queried_pt_weight = 1\n",
    "                else:\n",
    "                    queried_pc_labels = self.input_labels[split][cloud_idx][query_idx]\n",
    "                    queried_pc_labels = np.array([self.label_to_idx[l] for l in queried_pc_labels])\n",
    "                    queried_pt_weight = np.array([self.class_weight[split][0][n] for n in queried_pc_labels])\n",
    "\n",
    "                # Update the possibility of the selected points\n",
    "                dists = np.sum(np.square((points[query_idx] - pick_point).astype(np.float32)), axis=1)\n",
    "                delta = np.square(1 - dists / np.max(dists)) * queried_pt_weight\n",
    "                self.possibility[split][cloud_idx][query_idx] += delta\n",
    "                self.min_possibility[split][cloud_idx] = float(np.min(self.possibility[split][cloud_idx]))\n",
    "\n",
    "                if True:\n",
    "                    yield (queried_pc_xyz,\n",
    "                           queried_pc_colors.astype(np.float32),\n",
    "                           queried_pc_labels,\n",
    "                           query_idx.astype(np.int32),\n",
    "                           np.array([cloud_idx], dtype=np.int32))\n",
    "\n",
    "        gen_func = spatially_regular_gen\n",
    "        gen_types = (tf.float32, tf.float32, tf.int32, tf.int32, tf.int32)\n",
    "        gen_shapes = ([None, 3], [None, 3], [None], [None], [None])\n",
    "        return gen_func, gen_types, gen_shapes\n",
    "\n",
    "    def get_tf_mapping(self):\n",
    "        # Collect flat inputs\n",
    "        def tf_map(batch_xyz, batch_features, batch_labels, batch_pc_idx, batch_cloud_idx):\n",
    "            batch_features = tf.map_fn(self.tf_augment_input, [batch_xyz, batch_features], dtype=tf.float32)\n",
    "            input_points = []\n",
    "            input_neighbors = []\n",
    "            input_pools = []\n",
    "            input_up_samples = []\n",
    "\n",
    "            for i in range(cfg.num_layers):\n",
    "                neigh_idx = tf.py_func(DP.knn_search, [batch_xyz, batch_xyz, cfg.k_n], tf.int32)\n",
    "                sub_points = batch_xyz[:, :tf.shape(batch_xyz)[1] // cfg.sub_sampling_ratio[i], :]\n",
    "                pool_i = neigh_idx[:, :tf.shape(batch_xyz)[1] // cfg.sub_sampling_ratio[i], :]\n",
    "                up_i = tf.py_func(DP.knn_search, [sub_points, batch_xyz, 1], tf.int32)\n",
    "                input_points.append(batch_xyz)\n",
    "                input_neighbors.append(neigh_idx)\n",
    "                input_pools.append(pool_i)\n",
    "                input_up_samples.append(up_i)\n",
    "                batch_xyz = sub_points\n",
    "\n",
    "            input_list = input_points + input_neighbors + input_pools + input_up_samples\n",
    "            input_list += [batch_features, batch_labels, batch_pc_idx, batch_cloud_idx]\n",
    "\n",
    "            return input_list\n",
    "\n",
    "        return tf_map\n",
    "\n",
    "    # data augmentation\n",
    "    @staticmethod\n",
    "    def tf_augment_input(inputs):\n",
    "        xyz = inputs[0]\n",
    "        features = inputs[1]\n",
    "        theta = tf.random_uniform((1,), minval=0, maxval=2 * np.pi)\n",
    "        # Rotation matrices\n",
    "        c, s = tf.cos(theta), tf.sin(theta)\n",
    "        cs0 = tf.zeros_like(c)\n",
    "        cs1 = tf.ones_like(c)\n",
    "        R = tf.stack([c, -s, cs0, s, c, cs0, cs0, cs0, cs1], axis=1)\n",
    "        stacked_rots = tf.reshape(R, (3, 3))\n",
    "\n",
    "        # Apply rotations\n",
    "        transformed_xyz = tf.reshape(tf.matmul(xyz, stacked_rots), [-1, 3])\n",
    "        # Choose random scales for each example\n",
    "        min_s = cfg.augment_scale_min\n",
    "        max_s = cfg.augment_scale_max\n",
    "        if cfg.augment_scale_anisotropic:\n",
    "            s = tf.random_uniform((1, 3), minval=min_s, maxval=max_s)\n",
    "        else:\n",
    "            s = tf.random_uniform((1, 1), minval=min_s, maxval=max_s)\n",
    "\n",
    "        symmetries = []\n",
    "        for i in range(3):\n",
    "            if cfg.augment_symmetries[i]:\n",
    "                symmetries.append(tf.round(tf.random_uniform((1, 1))) * 2 - 1)\n",
    "            else:\n",
    "                symmetries.append(tf.ones([1, 1], dtype=tf.float32))\n",
    "        s *= tf.concat(symmetries, 1)\n",
    "\n",
    "        # Create N x 3 vector of scales to multiply with stacked_points\n",
    "        stacked_scales = tf.tile(s, [tf.shape(transformed_xyz)[0], 1])\n",
    "\n",
    "        # Apply scales\n",
    "        transformed_xyz = transformed_xyz * stacked_scales\n",
    "\n",
    "        noise = tf.random_normal(tf.shape(transformed_xyz), stddev=cfg.augment_noise)\n",
    "        transformed_xyz = transformed_xyz + noise\n",
    "        rgb = features[:, :3]\n",
    "        stacked_features = tf.concat([transformed_xyz, rgb], axis=-1)\n",
    "        return stacked_features\n",
    "\n",
    "    def init_input_pipeline(self):\n",
    "        print('Initiating input pipelines')\n",
    "        cfg.ignored_label_inds = [self.label_to_idx[ign_label] for ign_label in self.ignored_labels]\n",
    "        gen_function, gen_types, gen_shapes = self.get_batch_gen('training')\n",
    "        gen_function_val, _, _ = self.get_batch_gen('validation')\n",
    "        gen_function_test, _, _ = self.get_batch_gen('test')\n",
    "        self.train_data = tf.data.Dataset.from_generator(gen_function, gen_types, gen_shapes)\n",
    "        self.val_data = tf.data.Dataset.from_generator(gen_function_val, gen_types, gen_shapes)\n",
    "        self.test_data = tf.data.Dataset.from_generator(gen_function_test, gen_types, gen_shapes)\n",
    "\n",
    "        self.batch_train_data = self.train_data.batch(cfg.batch_size)\n",
    "        self.batch_val_data = self.val_data.batch(cfg.val_batch_size)\n",
    "        self.batch_test_data = self.test_data.batch(cfg.val_batch_size)\n",
    "        map_func = self.get_tf_mapping()\n",
    "\n",
    "        self.batch_train_data = self.batch_train_data.map(map_func=map_func)\n",
    "        self.batch_val_data = self.batch_val_data.map(map_func=map_func)\n",
    "        self.batch_test_data = self.batch_test_data.map(map_func=map_func)\n",
    "\n",
    "        self.batch_train_data = self.batch_train_data.prefetch(cfg.batch_size)\n",
    "        self.batch_val_data = self.batch_val_data.prefetch(cfg.val_batch_size)\n",
    "        self.batch_test_data = self.batch_test_data.prefetch(cfg.val_batch_size)\n",
    "\n",
    "        iter = tf.data.Iterator.from_structure(self.batch_train_data.output_types, self.batch_train_data.output_shapes)\n",
    "        self.flat_inputs = iter.get_next()\n",
    "        self.train_init_op = iter.make_initializer(self.batch_train_data)\n",
    "        self.val_init_op = iter.make_initializer(self.batch_val_data)\n",
    "        self.test_init_op = iter.make_initializer(self.batch_test_data)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--gpu', type=int, default=0, help='the number of GPUs to use [default: 0]')\n",
    "    parser.add_argument('--mode', type=str, default='train', help='options: train, test, vis')\n",
    "    parser.add_argument('--model_path', type=str, default='None', help='pretrained model path')\n",
    "    FLAGS = parser.parse_args()\n",
    "\n",
    "    GPU_ID = FLAGS.gpu\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(GPU_ID)\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "    Mode = FLAGS.mode\n",
    "    dataset = Semantic3D()\n",
    "    dataset.init_input_pipeline()\n",
    "\n",
    "    if Mode == 'train':\n",
    "        model = Network(dataset, cfg)\n",
    "        model.train(dataset)\n",
    "    elif Mode == 'test':\n",
    "        cfg.saving = False\n",
    "        model = Network(dataset, cfg)\n",
    "        if FLAGS.model_path is not 'None':\n",
    "            chosen_snap = FLAGS.model_path\n",
    "        else:\n",
    "            chosen_snapshot = -1\n",
    "            logs = np.sort([os.path.join('results', f) for f in os.listdir('results') if f.startswith('Log')])\n",
    "            chosen_folder = logs[-1]\n",
    "            snap_path = join(chosen_folder, 'snapshots')\n",
    "            snap_steps = [int(f[:-5].split('-')[-1]) for f in os.listdir(snap_path) if f[-5:] == '.meta']\n",
    "            chosen_step = np.sort(snap_steps)[-1]\n",
    "            chosen_snap = os.path.join(snap_path, 'snap-{:d}'.format(chosen_step))\n",
    "        tester = ModelTester(model, dataset, restore_snap=chosen_snap)\n",
    "        tester.test(model, dataset)\n",
    "\n",
    "    else:\n",
    "        ##################\n",
    "        # Visualize data #\n",
    "        ##################\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(dataset.train_init_op)\n",
    "            while True:\n",
    "                flat_inputs = sess.run(dataset.flat_inputs)\n",
    "                pc_xyz = flat_inputs[0]\n",
    "                sub_pc_xyz = flat_inputs[1]\n",
    "                labels = flat_inputs[21]\n",
    "                Plot.draw_pc_sem_ins(pc_xyz[0, :, :], labels[0, :])\n",
    "                Plot.draw_pc_sem_ins(sub_pc_xyz[0, :, :], labels[0, 0:np.shape(sub_pc_xyz)[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b35ec67",
   "metadata": {},
   "source": [
    "# Main semantic3D \n",
    "The main script loads the Semantic3D dataset and initializes the model using the configuration class. Based on the selected mode (train, test, or vis), it either trains RandLA-Net, evaluates it using IoU metrics, or visualizes point clouds with semantic labels. Data is loaded and augmented dynamically using TensorFlow's dataset API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ff4aedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:358: SyntaxWarning: \"is not\" with 'str' literal. Did you mean \"!=\"?\n",
      "<>:358: SyntaxWarning: \"is not\" with 'str' literal. Did you mean \"!=\"?\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'RandLANet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m join, exists\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mRandLANet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Network\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtester_Semantic3D\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelTester\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhelper_ply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m read_ply\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'RandLANet'"
     ]
    }
   ],
   "source": [
    "from os.path import join, exists\n",
    "from RandLANet import Network\n",
    "from tester_Semantic3D import ModelTester\n",
    "from helper_ply import read_ply\n",
    "from helper_tool import Plot\n",
    "from helper_tool import DataProcessing as DP\n",
    "from helper_tool import ConfigSemantic3D as cfg\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle, argparse, os\n",
    "\n",
    "\n",
    "class Semantic3D:\n",
    "    def __init__(self):\n",
    "        self.name = 'Semantic3D'\n",
    "        self.path = '/data/semantic3d'\n",
    "        self.label_to_names = {0: 'unlabeled',\n",
    "                               1: 'man-made terrain',\n",
    "                               2: 'natural terrain',\n",
    "                               3: 'high vegetation',\n",
    "                               4: 'low vegetation',\n",
    "                               5: 'buildings',\n",
    "                               6: 'hard scape',\n",
    "                               7: 'scanning artefacts',\n",
    "                               8: 'cars'}\n",
    "        self.num_classes = len(self.label_to_names)\n",
    "        self.label_values = np.sort([k for k, v in self.label_to_names.items()])\n",
    "        self.label_to_idx = {l: i for i, l in enumerate(self.label_values)}\n",
    "        self.ignored_labels = np.sort([0])\n",
    "\n",
    "        self.original_folder = join(self.path, 'original_data')\n",
    "        self.full_pc_folder = join(self.path, 'original_ply')\n",
    "        self.sub_pc_folder = join(self.path, 'input_{:.3f}'.format(cfg.sub_grid_size))\n",
    "\n",
    "        # Following KPConv to do the train-validation split\n",
    "        self.all_splits = [0, 1, 4, 5, 3, 4, 3, 0, 1, 2, 3, 4, 2, 0, 5]\n",
    "        self.val_split = 1\n",
    "\n",
    "        # Initial training-validation-testing files\n",
    "        self.train_files = []\n",
    "        self.val_files = []\n",
    "        self.test_files = []\n",
    "        cloud_names = [file_name[:-4] for file_name in os.listdir(self.original_folder) if file_name[-4:] == '.txt']\n",
    "        for pc_name in cloud_names:\n",
    "            if exists(join(self.original_folder, pc_name + '.labels')):\n",
    "                self.train_files.append(join(self.sub_pc_folder, pc_name + '.ply'))\n",
    "            else:\n",
    "                self.test_files.append(join(self.full_pc_folder, pc_name + '.ply'))\n",
    "\n",
    "        self.train_files = np.sort(self.train_files)\n",
    "        self.test_files = np.sort(self.test_files)\n",
    "\n",
    "        for i, file_path in enumerate(self.train_files):\n",
    "            if self.all_splits[i] == self.val_split:\n",
    "                self.val_files.append(file_path)\n",
    "\n",
    "        self.train_files = np.sort([x for x in self.train_files if x not in self.val_files])\n",
    "\n",
    "        # Initiate containers\n",
    "        self.val_proj = []\n",
    "        self.val_labels = []\n",
    "        self.test_proj = []\n",
    "        self.test_labels = []\n",
    "\n",
    "        self.possibility = {}\n",
    "        self.min_possibility = {}\n",
    "        self.class_weight = {}\n",
    "        self.input_trees = {'training': [], 'validation': [], 'test': []}\n",
    "        self.input_colors = {'training': [], 'validation': [], 'test': []}\n",
    "        self.input_labels = {'training': [], 'validation': []}\n",
    "\n",
    "        # Ascii files dict for testing\n",
    "        self.ascii_files = {\n",
    "            'MarketplaceFeldkirch_Station4_rgb_intensity-reduced.ply': 'marketsquarefeldkirch4-reduced.labels',\n",
    "            'sg27_station10_rgb_intensity-reduced.ply': 'sg27_10-reduced.labels',\n",
    "            'sg28_Station2_rgb_intensity-reduced.ply': 'sg28_2-reduced.labels',\n",
    "            'StGallenCathedral_station6_rgb_intensity-reduced.ply': 'stgallencathedral6-reduced.labels',\n",
    "            'birdfountain_station1_xyz_intensity_rgb.ply': 'birdfountain1.labels',\n",
    "            'castleblatten_station1_intensity_rgb.ply': 'castleblatten1.labels',\n",
    "            'castleblatten_station5_xyz_intensity_rgb.ply': 'castleblatten5.labels',\n",
    "            'marketplacefeldkirch_station1_intensity_rgb.ply': 'marketsquarefeldkirch1.labels',\n",
    "            'marketplacefeldkirch_station4_intensity_rgb.ply': 'marketsquarefeldkirch4.labels',\n",
    "            'marketplacefeldkirch_station7_intensity_rgb.ply': 'marketsquarefeldkirch7.labels',\n",
    "            'sg27_station10_intensity_rgb.ply': 'sg27_10.labels',\n",
    "            'sg27_station3_intensity_rgb.ply': 'sg27_3.labels',\n",
    "            'sg27_station6_intensity_rgb.ply': 'sg27_6.labels',\n",
    "            'sg27_station8_intensity_rgb.ply': 'sg27_8.labels',\n",
    "            'sg28_station2_intensity_rgb.ply': 'sg28_2.labels',\n",
    "            'sg28_station5_xyz_intensity_rgb.ply': 'sg28_5.labels',\n",
    "            'stgallencathedral_station1_intensity_rgb.ply': 'stgallencathedral1.labels',\n",
    "            'stgallencathedral_station3_intensity_rgb.ply': 'stgallencathedral3.labels',\n",
    "            'stgallencathedral_station6_intensity_rgb.ply': 'stgallencathedral6.labels'}\n",
    "\n",
    "        self.load_sub_sampled_clouds(cfg.sub_grid_size)\n",
    "\n",
    "    def load_sub_sampled_clouds(self, sub_grid_size):\n",
    "\n",
    "        tree_path = join(self.path, 'input_{:.3f}'.format(sub_grid_size))\n",
    "        files = np.hstack((self.train_files, self.val_files, self.test_files))\n",
    "\n",
    "        for i, file_path in enumerate(files):\n",
    "            cloud_name = file_path.split('/')[-1][:-4]\n",
    "            print('Load_pc_' + str(i) + ': ' + cloud_name)\n",
    "            if file_path in self.val_files:\n",
    "                cloud_split = 'validation'\n",
    "            elif file_path in self.train_files:\n",
    "                cloud_split = 'training'\n",
    "            else:\n",
    "                cloud_split = 'test'\n",
    "\n",
    "            # Name of the input files\n",
    "            kd_tree_file = join(tree_path, '{:s}_KDTree.pkl'.format(cloud_name))\n",
    "            sub_ply_file = join(tree_path, '{:s}.ply'.format(cloud_name))\n",
    "\n",
    "            # read ply with data\n",
    "            data = read_ply(sub_ply_file)\n",
    "            sub_colors = np.vstack((data['red'], data['green'], data['blue'])).T\n",
    "            if cloud_split == 'test':\n",
    "                sub_labels = None\n",
    "            else:\n",
    "                sub_labels = data['class']\n",
    "\n",
    "            # Read pkl with search tree\n",
    "            with open(kd_tree_file, 'rb') as f:\n",
    "                search_tree = pickle.load(f)\n",
    "\n",
    "            self.input_trees[cloud_split] += [search_tree]\n",
    "            self.input_colors[cloud_split] += [sub_colors]\n",
    "            if cloud_split in ['training', 'validation']:\n",
    "                self.input_labels[cloud_split] += [sub_labels]\n",
    "\n",
    "        # Get validation and test re_projection indices\n",
    "        print('\\nPreparing reprojection indices for validation and test')\n",
    "\n",
    "        for i, file_path in enumerate(files):\n",
    "\n",
    "            # get cloud name and split\n",
    "            cloud_name = file_path.split('/')[-1][:-4]\n",
    "\n",
    "            # Validation projection and labels\n",
    "            if file_path in self.val_files:\n",
    "                proj_file = join(tree_path, '{:s}_proj.pkl'.format(cloud_name))\n",
    "                with open(proj_file, 'rb') as f:\n",
    "                    proj_idx, labels = pickle.load(f)\n",
    "                self.val_proj += [proj_idx]\n",
    "                self.val_labels += [labels]\n",
    "\n",
    "            # Test projection\n",
    "            if file_path in self.test_files:\n",
    "                proj_file = join(tree_path, '{:s}_proj.pkl'.format(cloud_name))\n",
    "                with open(proj_file, 'rb') as f:\n",
    "                    proj_idx, labels = pickle.load(f)\n",
    "                self.test_proj += [proj_idx]\n",
    "                self.test_labels += [labels]\n",
    "        print('finished')\n",
    "        return\n",
    "\n",
    "    # Generate the input data flow\n",
    "    def get_batch_gen(self, split):\n",
    "        if split == 'training':\n",
    "            num_per_epoch = cfg.train_steps * cfg.batch_size\n",
    "        elif split == 'validation':\n",
    "            num_per_epoch = cfg.val_steps * cfg.val_batch_size\n",
    "        elif split == 'test':\n",
    "            num_per_epoch = cfg.val_steps * cfg.val_batch_size\n",
    "\n",
    "        # Reset possibility\n",
    "        self.possibility[split] = []\n",
    "        self.min_possibility[split] = []\n",
    "        self.class_weight[split] = []\n",
    "\n",
    "        # Random initialize\n",
    "        for i, tree in enumerate(self.input_trees[split]):\n",
    "            self.possibility[split] += [np.random.rand(tree.data.shape[0]) * 1e-3]\n",
    "            self.min_possibility[split] += [float(np.min(self.possibility[split][-1]))]\n",
    "\n",
    "        if split != 'test':\n",
    "            _, num_class_total = np.unique(np.hstack(self.input_labels[split]), return_counts=True)\n",
    "            self.class_weight[split] += [np.squeeze([num_class_total / np.sum(num_class_total)], axis=0)]\n",
    "\n",
    "        def spatially_regular_gen():\n",
    "\n",
    "            # Generator loop\n",
    "            for i in range(num_per_epoch):  # num_per_epoch\n",
    "\n",
    "                # Choose the cloud with the lowest probability\n",
    "                cloud_idx = int(np.argmin(self.min_possibility[split]))\n",
    "\n",
    "                # choose the point with the minimum of possibility in the cloud as query point\n",
    "                point_ind = np.argmin(self.possibility[split][cloud_idx])\n",
    "\n",
    "                # Get all points within the cloud from tree structure\n",
    "                points = np.array(self.input_trees[split][cloud_idx].data, copy=False)\n",
    "\n",
    "                # Center point of input region\n",
    "                center_point = points[point_ind, :].reshape(1, -1)\n",
    "\n",
    "                # Add noise to the center point\n",
    "                noise = np.random.normal(scale=cfg.noise_init / 10, size=center_point.shape)\n",
    "                pick_point = center_point + noise.astype(center_point.dtype)\n",
    "                query_idx = self.input_trees[split][cloud_idx].query(pick_point, k=cfg.num_points)[1][0]\n",
    "\n",
    "                # Shuffle index\n",
    "                query_idx = DP.shuffle_idx(query_idx)\n",
    "\n",
    "                # Get corresponding points and colors based on the index\n",
    "                queried_pc_xyz = points[query_idx]\n",
    "                queried_pc_xyz[:, 0:2] = queried_pc_xyz[:, 0:2] - pick_point[:, 0:2]\n",
    "                queried_pc_colors = self.input_colors[split][cloud_idx][query_idx]\n",
    "                if split == 'test':\n",
    "                    queried_pc_labels = np.zeros(queried_pc_xyz.shape[0])\n",
    "                    queried_pt_weight = 1\n",
    "                else:\n",
    "                    queried_pc_labels = self.input_labels[split][cloud_idx][query_idx]\n",
    "                    queried_pc_labels = np.array([self.label_to_idx[l] for l in queried_pc_labels])\n",
    "                    queried_pt_weight = np.array([self.class_weight[split][0][n] for n in queried_pc_labels])\n",
    "\n",
    "                # Update the possibility of the selected points\n",
    "                dists = np.sum(np.square((points[query_idx] - pick_point).astype(np.float32)), axis=1)\n",
    "                delta = np.square(1 - dists / np.max(dists)) * queried_pt_weight\n",
    "                self.possibility[split][cloud_idx][query_idx] += delta\n",
    "                self.min_possibility[split][cloud_idx] = float(np.min(self.possibility[split][cloud_idx]))\n",
    "\n",
    "                if True:\n",
    "                    yield (queried_pc_xyz,\n",
    "                           queried_pc_colors.astype(np.float32),\n",
    "                           queried_pc_labels,\n",
    "                           query_idx.astype(np.int32),\n",
    "                           np.array([cloud_idx], dtype=np.int32))\n",
    "\n",
    "        gen_func = spatially_regular_gen\n",
    "        gen_types = (tf.float32, tf.float32, tf.int32, tf.int32, tf.int32)\n",
    "        gen_shapes = ([None, 3], [None, 3], [None], [None], [None])\n",
    "        return gen_func, gen_types, gen_shapes\n",
    "\n",
    "    def get_tf_mapping(self):\n",
    "        # Collect flat inputs\n",
    "        def tf_map(batch_xyz, batch_features, batch_labels, batch_pc_idx, batch_cloud_idx):\n",
    "            batch_features = tf.map_fn(self.tf_augment_input, [batch_xyz, batch_features], dtype=tf.float32)\n",
    "            input_points = []\n",
    "            input_neighbors = []\n",
    "            input_pools = []\n",
    "            input_up_samples = []\n",
    "\n",
    "            for i in range(cfg.num_layers):\n",
    "                neigh_idx = tf.py_func(DP.knn_search, [batch_xyz, batch_xyz, cfg.k_n], tf.int32)\n",
    "                sub_points = batch_xyz[:, :tf.shape(batch_xyz)[1] // cfg.sub_sampling_ratio[i], :]\n",
    "                pool_i = neigh_idx[:, :tf.shape(batch_xyz)[1] // cfg.sub_sampling_ratio[i], :]\n",
    "                up_i = tf.py_func(DP.knn_search, [sub_points, batch_xyz, 1], tf.int32)\n",
    "                input_points.append(batch_xyz)\n",
    "                input_neighbors.append(neigh_idx)\n",
    "                input_pools.append(pool_i)\n",
    "                input_up_samples.append(up_i)\n",
    "                batch_xyz = sub_points\n",
    "\n",
    "            input_list = input_points + input_neighbors + input_pools + input_up_samples\n",
    "            input_list += [batch_features, batch_labels, batch_pc_idx, batch_cloud_idx]\n",
    "\n",
    "            return input_list\n",
    "\n",
    "        return tf_map\n",
    "\n",
    "    # data augmentation\n",
    "    @staticmethod\n",
    "    def tf_augment_input(inputs):\n",
    "        xyz = inputs[0]\n",
    "        features = inputs[1]\n",
    "        theta = tf.random_uniform((1,), minval=0, maxval=2 * np.pi)\n",
    "        # Rotation matrices\n",
    "        c, s = tf.cos(theta), tf.sin(theta)\n",
    "        cs0 = tf.zeros_like(c)\n",
    "        cs1 = tf.ones_like(c)\n",
    "        R = tf.stack([c, -s, cs0, s, c, cs0, cs0, cs0, cs1], axis=1)\n",
    "        stacked_rots = tf.reshape(R, (3, 3))\n",
    "\n",
    "        # Apply rotations\n",
    "        transformed_xyz = tf.reshape(tf.matmul(xyz, stacked_rots), [-1, 3])\n",
    "        # Choose random scales for each example\n",
    "        min_s = cfg.augment_scale_min\n",
    "        max_s = cfg.augment_scale_max\n",
    "        if cfg.augment_scale_anisotropic:\n",
    "            s = tf.random_uniform((1, 3), minval=min_s, maxval=max_s)\n",
    "        else:\n",
    "            s = tf.random_uniform((1, 1), minval=min_s, maxval=max_s)\n",
    "\n",
    "        symmetries = []\n",
    "        for i in range(3):\n",
    "            if cfg.augment_symmetries[i]:\n",
    "                symmetries.append(tf.round(tf.random_uniform((1, 1))) * 2 - 1)\n",
    "            else:\n",
    "                symmetries.append(tf.ones([1, 1], dtype=tf.float32))\n",
    "        s *= tf.concat(symmetries, 1)\n",
    "\n",
    "        # Create N x 3 vector of scales to multiply with stacked_points\n",
    "        stacked_scales = tf.tile(s, [tf.shape(transformed_xyz)[0], 1])\n",
    "\n",
    "        # Apply scales\n",
    "        transformed_xyz = transformed_xyz * stacked_scales\n",
    "\n",
    "        noise = tf.random_normal(tf.shape(transformed_xyz), stddev=cfg.augment_noise)\n",
    "        transformed_xyz = transformed_xyz + noise\n",
    "        rgb = features[:, :3]\n",
    "        stacked_features = tf.concat([transformed_xyz, rgb], axis=-1)\n",
    "        return stacked_features\n",
    "\n",
    "    def init_input_pipeline(self):\n",
    "        print('Initiating input pipelines')\n",
    "        cfg.ignored_label_inds = [self.label_to_idx[ign_label] for ign_label in self.ignored_labels]\n",
    "        gen_function, gen_types, gen_shapes = self.get_batch_gen('training')\n",
    "        gen_function_val, _, _ = self.get_batch_gen('validation')\n",
    "        gen_function_test, _, _ = self.get_batch_gen('test')\n",
    "        self.train_data = tf.data.Dataset.from_generator(gen_function, gen_types, gen_shapes)\n",
    "        self.val_data = tf.data.Dataset.from_generator(gen_function_val, gen_types, gen_shapes)\n",
    "        self.test_data = tf.data.Dataset.from_generator(gen_function_test, gen_types, gen_shapes)\n",
    "\n",
    "        self.batch_train_data = self.train_data.batch(cfg.batch_size)\n",
    "        self.batch_val_data = self.val_data.batch(cfg.val_batch_size)\n",
    "        self.batch_test_data = self.test_data.batch(cfg.val_batch_size)\n",
    "        map_func = self.get_tf_mapping()\n",
    "\n",
    "        self.batch_train_data = self.batch_train_data.map(map_func=map_func)\n",
    "        self.batch_val_data = self.batch_val_data.map(map_func=map_func)\n",
    "        self.batch_test_data = self.batch_test_data.map(map_func=map_func)\n",
    "\n",
    "        self.batch_train_data = self.batch_train_data.prefetch(cfg.batch_size)\n",
    "        self.batch_val_data = self.batch_val_data.prefetch(cfg.val_batch_size)\n",
    "        self.batch_test_data = self.batch_test_data.prefetch(cfg.val_batch_size)\n",
    "\n",
    "        iter = tf.data.Iterator.from_structure(self.batch_train_data.output_types, self.batch_train_data.output_shapes)\n",
    "        self.flat_inputs = iter.get_next()\n",
    "        self.train_init_op = iter.make_initializer(self.batch_train_data)\n",
    "        self.val_init_op = iter.make_initializer(self.batch_val_data)\n",
    "        self.test_init_op = iter.make_initializer(self.batch_test_data)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--gpu', type=int, default=0, help='the number of GPUs to use [default: 0]')\n",
    "    parser.add_argument('--mode', type=str, default='train', help='options: train, test, vis')\n",
    "    parser.add_argument('--model_path', type=str, default='None', help='pretrained model path')\n",
    "    FLAGS = parser.parse_args()\n",
    "\n",
    "    GPU_ID = FLAGS.gpu\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(GPU_ID)\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "    Mode = FLAGS.mode\n",
    "    dataset = Semantic3D()\n",
    "    dataset.init_input_pipeline()\n",
    "\n",
    "    if Mode == 'train':\n",
    "        model = Network(dataset, cfg)\n",
    "        model.train(dataset)\n",
    "    elif Mode == 'test':\n",
    "        cfg.saving = False\n",
    "        model = Network(dataset, cfg)\n",
    "        if FLAGS.model_path is not 'None':\n",
    "            chosen_snap = FLAGS.model_path\n",
    "        else:\n",
    "            chosen_snapshot = -1\n",
    "            logs = np.sort([os.path.join('results', f) for f in os.listdir('results') if f.startswith('Log')])\n",
    "            chosen_folder = logs[-1]\n",
    "            snap_path = join(chosen_folder, 'snapshots')\n",
    "            snap_steps = [int(f[:-5].split('-')[-1]) for f in os.listdir(snap_path) if f[-5:] == '.meta']\n",
    "            chosen_step = np.sort(snap_steps)[-1]\n",
    "            chosen_snap = os.path.join(snap_path, 'snap-{:d}'.format(chosen_step))\n",
    "        tester = ModelTester(model, dataset, restore_snap=chosen_snap)\n",
    "        tester.test(model, dataset)\n",
    "\n",
    "    else:\n",
    "        ##################\n",
    "        # Visualize data #\n",
    "        ##################\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(dataset.train_init_op)\n",
    "            while True:\n",
    "                flat_inputs = sess.run(dataset.flat_inputs)\n",
    "                pc_xyz = flat_inputs[0]\n",
    "                sub_pc_xyz = flat_inputs[1]\n",
    "                labels = flat_inputs[21]\n",
    "                Plot.draw_pc_sem_ins(pc_xyz[0, :, :], labels[0, :])\n",
    "                Plot.draw_pc_sem_ins(sub_pc_xyz[0, :, :], labels[0, 0:np.shape(sub_pc_xyz)[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d4b59e",
   "metadata": {},
   "source": [
    "# Tester Semanatic3D\n",
    "The ModelTester class is responsible for evaluating RandLA-Net after training. It restores a trained model, runs it on validation or test sets, and calculates standard metrics like accuracy and mean IoU. Since RandLA-Net uses subsampled point clouds for efficiency, the tester reprojects the predictions back to the original dense cloud for meaningful evaluation and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed75f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "from os.path import exists, join\n",
    "from helper_ply import read_ply, write_ply\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "def log_string(out_str, log_out):\n",
    "    log_out.write(out_str + '\\n')\n",
    "    log_out.flush()\n",
    "    print(out_str)\n",
    "\n",
    "\n",
    "class ModelTester:\n",
    "    def __init__(self, model, dataset, restore_snap=None):\n",
    "        # Tensorflow Saver definition\n",
    "        my_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        self.saver = tf.train.Saver(my_vars, max_to_keep=100)\n",
    "\n",
    "        # Create a session for running Ops on the Graph.\n",
    "        on_cpu = False\n",
    "        if on_cpu:\n",
    "            c_proto = tf.ConfigProto(device_count={'GPU': 0})\n",
    "        else:\n",
    "            c_proto = tf.ConfigProto()\n",
    "            c_proto.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=c_proto)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        if restore_snap is not None:\n",
    "            self.saver.restore(self.sess, restore_snap)\n",
    "            print(\"Model restored from \" + restore_snap)\n",
    "\n",
    "        # Add a softmax operation for predictions\n",
    "        self.prob_logits = tf.nn.softmax(model.logits)\n",
    "        self.test_probs = [np.zeros((l.data.shape[0], model.config.num_classes), dtype=np.float16)\n",
    "                           for l in dataset.input_trees['test']]\n",
    "\n",
    "        self.log_out = open('log_test_' + dataset.name + '.txt', 'a')\n",
    "\n",
    "    def test(self, model, dataset, num_votes=100):\n",
    "\n",
    "        # Smoothing parameter for votes\n",
    "        test_smooth = 0.98\n",
    "\n",
    "        # Initialise iterator with train data\n",
    "        self.sess.run(dataset.test_init_op)\n",
    "\n",
    "        # Test saving path\n",
    "        saving_path = time.strftime('results/Log_%Y-%m-%d_%H-%M-%S', time.gmtime())\n",
    "        test_path = join('test', saving_path.split('/')[-1])\n",
    "        makedirs(test_path) if not exists(test_path) else None\n",
    "        makedirs(join(test_path, 'predictions')) if not exists(join(test_path, 'predictions')) else None\n",
    "        makedirs(join(test_path, 'probs')) if not exists(join(test_path, 'probs')) else None\n",
    "\n",
    "        #####################\n",
    "        # Network predictions\n",
    "        #####################\n",
    "\n",
    "        step_id = 0\n",
    "        epoch_id = 0\n",
    "        last_min = -0.5\n",
    "\n",
    "        while last_min < num_votes:\n",
    "\n",
    "            try:\n",
    "                ops = (self.prob_logits,\n",
    "                       model.labels,\n",
    "                       model.inputs['input_inds'],\n",
    "                       model.inputs['cloud_inds'],)\n",
    "\n",
    "                stacked_probs, stacked_labels, point_idx, cloud_idx = self.sess.run(ops, {model.is_training: False})\n",
    "                stacked_probs = np.reshape(stacked_probs, [model.config.val_batch_size, model.config.num_points,\n",
    "                                                           model.config.num_classes])\n",
    "\n",
    "                for j in range(np.shape(stacked_probs)[0]):\n",
    "                    probs = stacked_probs[j, :, :]\n",
    "                    inds = point_idx[j, :]\n",
    "                    c_i = cloud_idx[j][0]\n",
    "                    self.test_probs[c_i][inds] = test_smooth * self.test_probs[c_i][inds] + (1 - test_smooth) * probs\n",
    "                step_id += 1\n",
    "                log_string('Epoch {:3d}, step {:3d}. min possibility = {:.1f}'.format(epoch_id, step_id, np.min(\n",
    "                    dataset.min_possibility['test'])), self.log_out)\n",
    "\n",
    "            except tf.errors.OutOfRangeError:\n",
    "\n",
    "                # Save predicted cloud\n",
    "                new_min = np.min(dataset.min_possibility['test'])\n",
    "                log_string('Epoch {:3d}, end. Min possibility = {:.1f}'.format(epoch_id, new_min), self.log_out)\n",
    "\n",
    "                if last_min + 4 < new_min:\n",
    "\n",
    "                    print('Saving clouds')\n",
    "\n",
    "                    # Update last_min\n",
    "                    last_min = new_min\n",
    "\n",
    "                    # Project predictions\n",
    "                    print('\\nReproject Vote #{:d}'.format(int(np.floor(new_min))))\n",
    "                    t1 = time.time()\n",
    "                    files = dataset.test_files\n",
    "                    i_test = 0\n",
    "                    for i, file_path in enumerate(files):\n",
    "                        # Get file\n",
    "                        points = self.load_evaluation_points(file_path)\n",
    "                        points = points.astype(np.float16)\n",
    "\n",
    "                        # Reproject probs\n",
    "                        probs = np.zeros(shape=[np.shape(points)[0], 8], dtype=np.float16)\n",
    "                        proj_index = dataset.test_proj[i_test]\n",
    "\n",
    "                        probs = self.test_probs[i_test][proj_index, :]\n",
    "\n",
    "                        # Insert false columns for ignored labels\n",
    "                        probs2 = probs\n",
    "                        for l_ind, label_value in enumerate(dataset.label_values):\n",
    "                            if label_value in dataset.ignored_labels:\n",
    "                                probs2 = np.insert(probs2, l_ind, 0, axis=1)\n",
    "\n",
    "                        # Get the predicted labels\n",
    "                        preds = dataset.label_values[np.argmax(probs2, axis=1)].astype(np.uint8)\n",
    "\n",
    "                        # Save plys\n",
    "                        cloud_name = file_path.split('/')[-1]\n",
    "\n",
    "                        # Save ascii preds\n",
    "                        ascii_name = join(test_path, 'predictions', dataset.ascii_files[cloud_name])\n",
    "                        np.savetxt(ascii_name, preds, fmt='%d')\n",
    "                        log_string(ascii_name + 'has saved', self.log_out)\n",
    "                        i_test += 1\n",
    "\n",
    "                    t2 = time.time()\n",
    "                    print('Done in {:.1f} s\\n'.format(t2 - t1))\n",
    "                    self.sess.close()\n",
    "                    return\n",
    "\n",
    "                self.sess.run(dataset.test_init_op)\n",
    "                epoch_id += 1\n",
    "                step_id = 0\n",
    "                continue\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def load_evaluation_points(file_path):\n",
    "        data = read_ply(file_path)\n",
    "        return np.vstack((data['x'], data['y'], data['z'])).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f524008",
   "metadata": {},
   "source": [
    "RandLA-Net offers significantly more architectural complexity than PointNet or decision trees. It uses attention and multiscale neighborhood aggregation to achieve better spatial understanding of point clouds. However, it requires more preprocessing, data structures (k-d trees), and TensorFlow-specific utilities, making it harder to adapt without full dataset and config structure. When working with raw road scenes, a simplified model like PointNet may be more flexible, but RandLA-Net is more scalable and powerful for high-resolution labeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
