{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c7fc1a",
   "metadata": {},
   "source": [
    "# Road Point Cloud Classification\n",
    "1. Data Loader\n",
    "2. Tree-based methods and SVM\n",
    "3. Convolutional Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ea1e0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minterpolate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m griddata\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Visualization\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# Basic data processing & preparation functionality\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import griddata\n",
    "import cv2\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.colors as pc\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import open3d as o3d\n",
    "\n",
    "# Measurements of time to fit a model\n",
    "import time\n",
    "\n",
    "#   Tree-based methods\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#   SVM related methods\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Standard test dataset\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dataset tuning\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Model tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Metric for model evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Deep learning methods\n",
    "from tqdm import tqdm\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "105f894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Road Point Cloud Classification - Data Loader --- (Jonas)\n",
    "\n",
    "# Function to load all point clouds from a folder\n",
    "def load_point_clouds(labels, folder_path=\"dataset/\"):\n",
    "    \"\"\"\n",
    "    Loads all .npy point cloud files from the specified folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing .npy files.\n",
    "\n",
    "    Returns:\n",
    "        list of np.ndarray: List of loaded point cloud arrays.\n",
    "    \"\"\"\n",
    "    point_clouds = []\n",
    "    for label in labels:\n",
    "        folder = f\"{folder_path}{label}\"  # Adjust the folder paths as needed\n",
    "        for filename in os.listdir(folder):\n",
    "            if filename.endswith('.npy'):\n",
    "                file_path = os.path.join(folder, filename)\n",
    "                data = np.load(file_path)\n",
    "                # print(data.shape)  # Print shape of each loaded point cloud\n",
    "                label_vector = np.zeros((data.shape[0], 1))  # Create a label vector of zeros\n",
    "                label_vector.fill(labels.index(label))  # Fill the label vector with the index of the label\n",
    "                data = np.hstack((data, label_vector))  # Concatenate label vector to the point cloud data\n",
    "                # print(data.shape)  # Print shape after concatenation\n",
    "                point_clouds.append(data)\n",
    "    return point_clouds\n",
    "\n",
    "def visualize_point_cloud(points, title=\"Point Cloud\", color='b', s=1):\n",
    "    \"\"\"\n",
    "    Visualizes a 3D point cloud using open3d.\n",
    "\n",
    "    Args:\n",
    "        points (np.ndarray): Array of shape (N, 3) or (N, >=3) with x, y, z coordinates.\n",
    "        title (str): Plot title.\n",
    "        color (str or array): Color of points.\n",
    "        s (float): Marker size.\n",
    "    \"\"\"\n",
    "    if points.shape[1] < 3:\n",
    "        raise ValueError(\"Point cloud must have at least 3 columns (x, y, z).\")\n",
    "    # Create an Open3D point cloud object\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    # Set the points\n",
    "    pcd.points = o3d.utility.Vector3dVector(points[:, :3])  # Use only the first three columns for x, y, z\n",
    "    # Set the colors if available\n",
    "    if points.shape[1] >= 6:\n",
    "        colors = points[:, 3:6] / 255.0  # Normalize RGB values to [0, 1]\n",
    "        pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    else:\n",
    "        pcd.paint_uniform_color(pc.hex_to_rgb(color))  # Use a single color if no RGB columns are present\n",
    "    # Visualize the point cloud\n",
    "    o3d.visualization.draw_geometries([pcd], window_name=title, width=800, height=600, point_show_normal=False)\n",
    "    \n",
    "\n",
    "\n",
    "def visualize_point_cloud_plotly(points, title=\"Point Cloud\", s=2):\n",
    "    \"\"\"\n",
    "    Visualizes a 3D point cloud using plotly.\n",
    "\n",
    "    Args:\n",
    "        points (np.ndarray): Array of shape (N, 3) or (N, >=3) with x, y, z coordinates.\n",
    "        title (str): Plot title.\n",
    "        s (float): Marker size.\n",
    "    \"\"\"\n",
    "    if points.shape[1] < 3:\n",
    "        raise ValueError(\"Point cloud must have at least 3 columns (x, y, z).\")\n",
    "    color = []\n",
    "    for i in range(points.shape[0]):\n",
    "        color.append(np.hstack((points[i,3] / 255, points[i,4] /255, points[i,5] / 255)))  \n",
    "    fig = px.scatter_3d(\n",
    "        x=points[:, 0], y=points[:, 1], z=points[:, 2], color=color,\n",
    "        title=title,\n",
    "        opacity=0.8\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=s))\n",
    "    fig.update_layout(scene=dict(\n",
    "        xaxis_title='X', yaxis_title='Y', zaxis_title='Z'\n",
    "    ))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c30f53",
   "metadata": {},
   "source": [
    "# Tree-based methods and SVM\n",
    "\n",
    "For tree-based methods and support vector machines, a one-dimensional feature vector is needed as input data for every sample of the dataset. Therefore, in a first, basic approach, we simply downsampled each point cloud randomly to the number of points of the smallest point cloud in the dataset. Another option would be padding all samples of the dataset by adding dummy points to match the shape of the point cloud with the most points.\n",
    "\n",
    "Afterwards, we prepared the input feature matrix X by simply concatenating all selected features for all the points for each point cloud. Selecting different features could also give different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfdbee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Preparation for tree-based methods and SVM --- (Jonas)\n",
    "\n",
    "def downsample_point_clouds(dataset):\n",
    "    \"\"\"\n",
    "    Downsamples all point clouds in the dataset to the size of the first point cloud.\n",
    "\n",
    "    Args:\n",
    "        dataset (list of np.ndarray): List of point clouds.\n",
    "\n",
    "    Returns:\n",
    "        list of np.ndarray: List of downsampled point clouds.\n",
    "    \"\"\"\n",
    "    if not dataset:\n",
    "        return []\n",
    "\n",
    "    # Sort the dataset by the number of points in each point cloud\n",
    "    dataset.sort(key=lambda x: x.shape[0])\n",
    "    print(f\"Shape of smallest point cloud: {dataset[0].shape}\")  # Print the shape of the first point cloud\n",
    "    print(f\"Shape of largest point cloud: {dataset[-1].shape}\")  # Print the shape of the last point cloud\n",
    "\n",
    "    # Downsample all point clouds to the size of the first point cloud\n",
    "    for i in range(len(dataset)):\n",
    "        sample = dataset[i]  # Get the current sample\n",
    "        indices = np.random.choice(sample.shape[0], size=dataset[0].shape[0], replace=False)  # Randomly select indices\n",
    "        sample_downsampled = sample[indices]  # Downsample the point cloud to match the size of the first point cloud \n",
    "        dataset[i] = sample_downsampled  # Update the dataset with the downsampled sample\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def pad_point_clouds(dataset):\n",
    "    \"\"\"\n",
    "    Pads all point clouds in the dataset to the size of the largest point cloud.\n",
    "\n",
    "    Args:\n",
    "        dataset (list of np.ndarray): List of point clouds.\n",
    "\n",
    "    Returns:\n",
    "        list of np.ndarray: List of padded point clouds.\n",
    "    \"\"\"\n",
    "    if not dataset:\n",
    "        return []\n",
    "\n",
    "    max_size = max(sample.shape[0] for sample in dataset)  # Find the maximum size of point clouds\n",
    "    padded_dataset = []\n",
    "\n",
    "    for sample in dataset:\n",
    "        if sample.shape[0] < max_size:\n",
    "            padding = np.zeros((max_size - sample.shape[0], sample.shape[1]))  # Create padding\n",
    "            padded_sample = np.vstack((sample, padding))  # Stack the original sample with padding\n",
    "        else:\n",
    "            padded_sample = sample  # No padding needed\n",
    "        padded_dataset.append(padded_sample)  # Append the padded sample to the new dataset\n",
    "\n",
    "    return padded_dataset\n",
    "\n",
    "\n",
    "def prepare_feature_vectors(dataset):\n",
    "    \"\"\"\n",
    "    Prepares feature vectors and labels from the dataset of point clouds.\n",
    "\n",
    "    Args:\n",
    "        dataset (list of np.ndarray): List of downsampled point clouds.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Feature vectors (X) and labels (y).\n",
    "    \"\"\"\n",
    "    # Convert the list of point clouds to a single NumPy array\n",
    "    feature_vectors = []\n",
    "    target_labels = []  # Initialize a list to store labels\n",
    "    for sample in dataset:\n",
    "        feature_vector = np.array([])  # Initialize an empty array for the feature vector\n",
    "        # Iterate over each point in the sample and concatenate its features to the feature vector\n",
    "        for i in range(sample.shape[0]):\n",
    "            feature_vector = np.hstack((feature_vector, sample[i, :-1]))\n",
    "        target_labels.append(sample[i, -1])  \n",
    "        # Concatenate all features except the label\n",
    "        feature_vectors.append(feature_vector)  # Append the feature vector to the list\n",
    "        \n",
    "    return np.vstack(feature_vectors), np.hstack(target_labels)  # Return stacked feature vectors and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3e29c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1425 point clouds from 6 categories.\n",
      "Shape of smallest point cloud: (510, 23)\n",
      "Shape of largest point cloud: (16661, 23)\n",
      "Dataset shape: (1425, 11220), Labels shape: (1425,)\n"
     ]
    }
   ],
   "source": [
    "# Load all point clouds from each category\n",
    "labels = [\"2lanes\", \"3lanes\", \"crossing\", \"split4lanes\", \"split6lanes\", \"transition\"]\n",
    "folder_path = \"C:/Users/jonas/OneDrive/Dokumente/Studium/Master/SoSe 2025/Machine Learning for Civil Engineering/Group Project/4_Road_point_cloud_classification/dataset/\"  # Adjust the folder path as needed\n",
    "dataset = load_point_clouds(labels, folder_path)  # Adjust the folder path as needed\n",
    "print(f\"Loaded {len(dataset)} point clouds from {len(labels)} categories.\")\n",
    "\n",
    "# Optionally visualize the first point cloud from each category\n",
    "for i, label in enumerate(labels):\n",
    "    if dataset:\n",
    "        visualize_point_cloud(dataset[i], title=f\"Point Cloud - {label} - {dataset[i].shape[0]} points\")\n",
    "    else:\n",
    "        print(f\"No point clouds found for label: {label}\")\n",
    "\n",
    "# Downsample the point clouds to the size of the first point cloud\n",
    "dataset = downsample_point_clouds(dataset) \n",
    "# Prepare feature vectors and labels\n",
    "X, y = prepare_feature_vectors(dataset)\n",
    "# Print the shape of the dataset\n",
    "print(f\"Dataset shape: {X.shape}, Labels shape: {y.shape}\")\n",
    "# Perform a train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea4be251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Accuracy: 59.38375350140056%\n",
      "Cross-Validation results: Mean accuracy: 56.35087719298245%, Standard deviation: 1.290159741112293%\n"
     ]
    }
   ],
   "source": [
    "# --- Random Forest Classifier with Cross-Validation ---\n",
    "\n",
    "# Initialize the Random Forest Classifier with specified parameters\n",
    "randomforest = RandomForestClassifier(n_estimators = 300, criterion = \"gini\", max_depth=3, random_state=0)\n",
    "# Fit the model to the training data\n",
    "randomforest.fit(X_train, y_train)\n",
    "# Evaluate the model on the test data\n",
    "y_pred = randomforest.predict(X_test)\n",
    "# Calculate the accuracy of the model\n",
    "acc_rf = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Classifier Accuracy: {acc_rf * 100}%\")\n",
    "\n",
    "# Perform cross-validation (CV) with 3 folds\n",
    "# Initialize KFold with 3 splits, no random state, and shuffling enabled\n",
    "cv = KFold(n_splits=3, random_state=None, shuffle=True)\n",
    "# Use the cross_val_score function to perform CV on the training data \n",
    "scores_rf = cross_val_score(randomforest, X,  y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# print out the mean accuracy and standard deviation over the 3 folds\n",
    "cv_mean = np.mean(np.array(scores_rf))\n",
    "cv_std = np.std(np.array(scores_rf))\n",
    "print(f\"Cross-Validation results: Mean accuracy: {cv_mean * 100}%, Standard deviation: {cv_std * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "129ab73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning:\n",
      "\n",
      "The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Classifier Accuracy: 50.98039215686274%\n",
      "Cross-Validation results: Mean accuracy: 44.98245614035088%, Standard deviation: 0.694701399060467%\n"
     ]
    }
   ],
   "source": [
    "# --- AdaBoost Classifier ---\n",
    "\n",
    "# Initialize the AdaBoost Classifier with the Decision Tree as base estimator\n",
    "adaboost = AdaBoostClassifier(n_estimators = 100, algorithm = \"SAMME\")\n",
    "# Fit the model to the training data\n",
    "adaboost.fit(X_train, y_train)\n",
    "# Predict the test set\n",
    "y_pred = adaboost.predict(X_test)\n",
    "# Calculate the accuracy of the model\n",
    "acc_adaboost = accuracy_score(y_test, y_pred)\n",
    "print(f\"AdaBoost Classifier Accuracy: {acc_adaboost * 100}%\")\n",
    "\n",
    "# Perform cross-validation (CV) with 3 folds\n",
    "# Initialize KFold with 3 splits, no random state, and shuffling enabled\n",
    "cv = KFold(n_splits=3, random_state=None, shuffle=True)\n",
    "# Use the cross_val_score function to perform CV on the training data\n",
    "scores_adaboost = cross_val_score(adaboost, X,  y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# print out the mean accuracy and standard deviation over the 3 folds\n",
    "cv_mean = np.mean(np.array(scores_adaboost))\n",
    "cv_std = np.std(np.array(scores_adaboost))\n",
    "print(f\"Cross-Validation results: Mean accuracy: {cv_mean * 100}%, Standard deviation: {cv_std * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "389584d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Classifier (One-vs-One Strategy) Accuracy : 43.69747899159664 %\n",
      "\n",
      "\n",
      "Cross-Validation results: Mean accuracy: 39.578947368421055%, Standard deviation: 1.0313641022244975%\n"
     ]
    }
   ],
   "source": [
    "# --- Support Vector Machine Classifier with One-vs-One Strategy ---\n",
    "\n",
    "# Apporach 1 \n",
    "svc_ovo  = OneVsOneClassifier(SVC(C=1, kernel=\"rbf\"))\n",
    "# Approach 2\n",
    "#clf = SVC(C=0.8, kernel=\"sigmoid\", decision_function_shape= \"ovo\")\n",
    "# Fit the model\n",
    "svc_ovo.fit(X_train,y_train)\n",
    "# Predict the test set\n",
    "predictions = svc_ovo .predict(X_test)\n",
    "# Compute accuracy\n",
    "acc_svc = accuracy_score(y_test, predictions)\n",
    "print(f\"Support Vector Classifier (One-vs-One Strategy) Accuracy : {acc_svc * 100} %\\n\\n\")\n",
    "\n",
    "# Perform cross-validation (CV) with 3 folds\n",
    "# Initialize KFold with 3 splits, no random state, and shuffling enabled\n",
    "cv = KFold(n_splits=3, random_state=None, shuffle=True)\n",
    "# Use the cross_val_score function to perform CV on the training data \n",
    "scores_svc = cross_val_score(svc_ovo, X,  y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# print out the mean accuracy and standard deviation over the 3 folds\n",
    "cv_mean = np.mean(np.array(scores_svc))\n",
    "cv_std = np.std(np.array(scores_svc))\n",
    "print(f\"Cross-Validation results: Mean accuracy: {cv_mean * 100}%, Standard deviation: {cv_std * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7660f461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Model  Accuracy  Cross-Validation Mean  Cross-Validation Std\n",
      "0   Random Forest  0.593838               0.563509              0.012902\n",
      "1       Ada Boost  0.509804               0.449825              0.006947\n",
      "2  SVM One-vs-One  0.436975               0.395789              0.010314\n"
     ]
    }
   ],
   "source": [
    "# Create a table with the results\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'Ada Boost', 'SVM One-vs-One'],\n",
    "    'Accuracy': [acc_rf, acc_adaboost, acc_svc],\n",
    "    'Cross-Validation Mean': [np.mean(np.array(scores_rf)), np.mean(np.array(scores_adaboost)), np.mean(np.array(scores_svc))],\n",
    "    'Cross-Validation Std': [np.std(np.array(scores_rf)), np.std(np.array(scores_adaboost)), np.std(np.array(scores_svc))]\n",
    "})  \n",
    "# Display the results table\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed34e070",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fd22c8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_point_clouds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m interpolated\n\u001b[32m     77\u001b[39m folder_path = \u001b[33m\"\u001b[39m\u001b[33mC:/Users/jonas/OneDrive/Dokumente/Studium/Master/SoSe 2025/Machine Learning for Civil Engineering/Group Project/4_Road_point_cloud_classification/dataset/\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Adjust the folder path as needed\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m dataset = \u001b[43mload_point_clouds\u001b[49m(labels, folder_path)\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Optionally visualize the first point cloud from each category as an image\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(labels):\n",
      "\u001b[31mNameError\u001b[39m: name 'load_point_clouds' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Data Preparation for Deep Learning --- (Jonas)\n",
    "\n",
    "def prepare_dataset():\n",
    "    # Load all point clouds from each category\n",
    "    labels = [\"2lanes\", \"3lanes\", \"crossing\", \"split4lanes\", \"split6lanes\", \"transition\"]\n",
    "    dataset = load_point_clouds(labels)\n",
    "    # Prepare feature vectors and labels for convolutional neural networks\n",
    "    dataset = downsample_point_clouds(dataset)\n",
    "    # Convert the list of point clouds to image-like tensors\n",
    "\n",
    "\n",
    "def convert_pc_to_image(pc, img_size=(200, 200)):\n",
    "    \"\"\"\n",
    "    Converts a point cloud to a 2D image representation.\n",
    "    \n",
    "    Args:\n",
    "        pc (np.ndarray): Point cloud of shape (N, 3) or (N, >=3).\n",
    "        img_size (tuple): Size of the output image (height, width).\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: 2D image representation of the point cloud.\n",
    "    \"\"\"\n",
    "    if pc.shape[1] < 3:\n",
    "        raise ValueError(\"Point cloud must have at least 3 columns (x, y, z).\")\n",
    "    \n",
    "    # Normalize the point cloud to fit within the image size\n",
    "    pc_normalized = (pc[:, :2] - np.min(pc[:, :2], axis=0)) / (np.max(pc[:, :2], axis=0) - np.min(pc[:, :2], axis=0))\n",
    "    pc_normalized *= img_size[0]  # Scale to image size\n",
    "    \n",
    "    # Create an empty image\n",
    "    img = np.zeros(img_size)\n",
    "    height, width = img_size\n",
    "\n",
    "    x, y, z, color = pc_normalized[:, 0], pc_normalized[:, 1], pc[:, 2], pc[:, 3:6] if pc.shape[1] >= 6 else np.zeros((pc.shape[0], 3)) \n",
    "\n",
    "    # --- Interpolation of Point Cloud Data for Visualization --- (Jonas)\n",
    "\n",
    "    # sort x,y,z by z in ascending order so the highest z is plotted over the lowest z\n",
    "    zSort = z.argsort()\n",
    "    x, y, z, color = x[zSort], y[zSort], z[zSort], color[zSort]\n",
    "\n",
    "    # interpolation\n",
    "    # generate a grid where the interpolation will be calculated\n",
    "    X, Y = np.meshgrid(np.arange(width), np.arange(height))\n",
    "\n",
    "    R = griddata(np.vstack((x, y)).T, color[:, 0], (X, Y), method='cubic')\n",
    "    Rlinear= griddata(np.vstack((x, y)).T, color[:, 0], (X, Y), method='nearest')\n",
    "    G = griddata(np.vstack((x, y)).T, color[:, 1], (X, Y), method='cubic')\n",
    "    Glinear= griddata(np.vstack((x, y)).T, color[:, 1], (X, Y), method='nearest')\n",
    "    B = griddata(np.vstack((x, y)).T, color[:, 2], (X, Y), method='cubic')\n",
    "    Blinear= griddata(np.vstack((x, y)).T, color[:, 2], (X, Y), method='nearest')\n",
    "\n",
    "    #Fill empty values with nearest neighbor\n",
    "    R[np.isnan(R)] = Rlinear[np.isnan(R)]\n",
    "    G[np.isnan(G)] = Glinear[np.isnan(G)]\n",
    "    B[np.isnan(B)] = Blinear[np.isnan(B)]\n",
    "\n",
    "    # Normalize the color channels to [0, 1]\n",
    "    R = R - np.min(R)\n",
    "    G = G - np.min(G)\n",
    "    B = B - np.min(B)\n",
    "    # Ensure no negative values\n",
    "    R[R < 0] = 0\n",
    "    G[G < 0] = 0\n",
    "    B[B < 0] = 0\n",
    "\n",
    "    R = R/np.max(R)\n",
    "    G = G/np.max(G)\n",
    "    B = B/np.max(B)\n",
    "\n",
    "    interpolated = cv2.merge((R, G, B))\n",
    "\n",
    "    return interpolated\n",
    "\n",
    "\n",
    "\n",
    "folder_path = \"C:/Users/jonas/OneDrive/Dokumente/Studium/Master/SoSe 2025/Machine Learning for Civil Engineering/Group Project/4_Road_point_cloud_classification/dataset/\"  # Adjust the folder path as needed\n",
    "dataset = load_point_clouds(labels, folder_path)\n",
    "\n",
    "# Optionally visualize the first point cloud from each category as an image\n",
    "for i, label in enumerate(labels):\n",
    "    if dataset:\n",
    "        img = convert_pc_to_image(dataset[i], img_size=(400, 400))  # Convert point cloud to image\n",
    "        #show the image\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Point Cloud - {label} - {dataset[i].shape[0]} points\")\n",
    "    else:\n",
    "        print(f\"No point clouds found for label: {label}\")\n",
    "\n",
    "# Load the dataset and prepare it for deep learning\n",
    "X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor, num_classes = prepare_dataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb36689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Building a Convolutional Neural Network --- (Jonas)\n",
    "def build_cnn(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Builds a Convolutional Neural Network (CNN) model.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Shape of the input data (channels, height, width).\n",
    "        num_classes (int): Number of output classes.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The CNN model.\n",
    "    \"\"\"\n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNN, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(in_channels=input_shape[0], out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "            self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "            self.fc1 = nn.Linear(64 * input_shape[1] * input_shape[2] // 4, 128)\n",
    "            self.fc2 = nn.Linear(128, num_classes)\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.pool(F.relu(self.conv1(x)))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "            x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "    return CNN()\n",
    "\n",
    "# Prepare the dataset for deep learning\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
