{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c7fc1a",
   "metadata": {},
   "source": [
    "# Road Point Cloud Classification\n",
    "1. Data Loader\n",
    "2. Tree-based methods and SVM\n",
    "3. Convolutional Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d982e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==2.2.6 in c:\\users\\jonas\\onedrive\\dokumente\\studium\\master\\sose_2025\\machine_learning\\group_project\\4_road_point_cloud_classification\\road-point-cloud-classification\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (2.2.6)\n",
      "Requirement already satisfied: pandas==2.3.1 in c:\\users\\jonas\\onedrive\\dokumente\\studium\\master\\sose_2025\\machine_learning\\group_project\\4_road_point_cloud_classification\\road-point-cloud-classification\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (2.3.1)\n",
      "Requirement already satisfied: scikit-learn==1.7.0 in c:\\users\\jonas\\onedrive\\dokumente\\studium\\master\\sose_2025\\machine_learning\\group_project\\4_road_point_cloud_classification\\road-point-cloud-classification\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (1.7.0)\n",
      "Collecting matplotlib==3.6.2 (from -r requirements.txt (line 5))\n",
      "  Downloading matplotlib-3.6.2.tar.gz (35.8 MB)\n",
      "     ---------------------------------------- 0.0/35.8 MB ? eta -:--:--\n",
      "     ----- ---------------------------------- 5.2/35.8 MB 28.9 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 9.7/35.8 MB 24.1 MB/s eta 0:00:02\n",
      "     -------------- ------------------------ 13.6/35.8 MB 22.7 MB/s eta 0:00:01\n",
      "     ------------------ -------------------- 16.8/35.8 MB 20.4 MB/s eta 0:00:01\n",
      "     ---------------------- ---------------- 20.4/35.8 MB 19.8 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 24.9/35.8 MB 19.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 30.4/35.8 MB 20.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 33.3/35.8 MB 19.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  35.7/35.8 MB 19.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  35.7/35.8 MB 19.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 35.8/35.8 MB 16.3 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting seaborn==0.12.1 (from -r requirements.txt (line 6))\n",
      "  Downloading seaborn-0.12.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: scipy==1.16.0 in c:\\users\\jonas\\onedrive\\dokumente\\studium\\master\\sose_2025\\machine_learning\\group_project\\4_road_point_cloud_classification\\road-point-cloud-classification\\.venv\\lib\\site-packages (from -r requirements.txt (line 7)) (1.16.0)\n",
      "Collecting plotly==5.10.0 (from -r requirements.txt (line 9))\n",
      "  Downloading plotly-5.10.0-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch==1.13.0 (from versions: 2.6.0, 2.7.0, 2.7.1)\n",
      "ERROR: No matching distribution found for torch==1.13.0\n"
     ]
    }
   ],
   "source": [
    "#install required packages\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41ea1e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data processing & preparation functionality\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import griddata\n",
    "import cv2\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.colors as pc\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "#import open3d as o3d\n",
    "\n",
    "# Measurements of time to fit a model\n",
    "import time\n",
    "\n",
    "#   Tree-based methods\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#   SVM related methods\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Standard test dataset\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dataset tuning\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Model tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Metric for model evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Deep learning methods\n",
    "from tqdm import tqdm\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay# --- Data Preparation for CNN --- (Jonas)\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "#from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105f894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Road Point Cloud Classification - Data Loader --- (Jonas)\n",
    "\n",
    "# Function to load all point clouds from a folder\n",
    "def load_point_clouds(labels, folder_path=\"dataset/\"):\n",
    "    \"\"\"\n",
    "    Loads all .npy point cloud files from the specified folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing .npy files.\n",
    "\n",
    "    Returns:\n",
    "        list of np.ndarray: List of loaded point cloud arrays.\n",
    "    \"\"\"\n",
    "    point_clouds = []\n",
    "    for label in labels:\n",
    "        folder = f\"{folder_path}{label}\"  # Adjust the folder paths as needed\n",
    "        for filename in os.listdir(folder):\n",
    "            if filename.endswith('.npy'):\n",
    "                file_path = os.path.join(folder, filename)\n",
    "                data = np.load(file_path)\n",
    "                # print(data.shape)  # Print shape of each loaded point cloud\n",
    "                label_vector = np.zeros((data.shape[0], 1))  # Create a label vector of zeros\n",
    "                label_vector.fill(labels.index(label))  # Fill the label vector with the index of the label\n",
    "                data = np.hstack((data, label_vector))  # Concatenate label vector to the point cloud data\n",
    "                # print(data.shape)  # Print shape after concatenation\n",
    "                point_clouds.append(data)\n",
    "    return point_clouds\n",
    "\n",
    "def visualize_point_cloud(points, title=\"Point Cloud\", color='b', s=1):\n",
    "    \"\"\"\n",
    "    Visualizes a 3D point cloud using open3d.\n",
    "\n",
    "    Args:\n",
    "        points (np.ndarray): Array of shape (N, 3) or (N, >=3) with x, y, z coordinates.\n",
    "        title (str): Plot title.\n",
    "        color (str or array): Color of points.\n",
    "        s (float): Marker size.\n",
    "    \"\"\"\n",
    "    if points.shape[1] < 3:\n",
    "        raise ValueError(\"Point cloud must have at least 3 columns (x, y, z).\")\n",
    "    # Create an Open3D point cloud object\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    # Set the points\n",
    "    pcd.points = o3d.utility.Vector3dVector(points[:, :3])  # Use only the first three columns for x, y, z\n",
    "    # Set the colors if available\n",
    "    if points.shape[1] >= 6:\n",
    "        colors = points[:, 3:6] / 255.0  # Normalize RGB values to [0, 1]\n",
    "        pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    else:\n",
    "        pcd.paint_uniform_color(pc.hex_to_rgb(color))  # Use a single color if no RGB columns are present\n",
    "    # Visualize the point cloud\n",
    "    o3d.visualization.draw_geometries([pcd], window_name=title, width=800, height=600, point_show_normal=False)\n",
    "    \n",
    "\n",
    "\n",
    "def visualize_point_cloud_plotly(points, title=\"Point Cloud\", s=2):\n",
    "    \"\"\"\n",
    "    Visualizes a 3D point cloud using plotly.\n",
    "\n",
    "    Args:\n",
    "        points (np.ndarray): Array of shape (N, 3) or (N, >=3) with x, y, z coordinates.\n",
    "        title (str): Plot title.\n",
    "        s (float): Marker size.\n",
    "    \"\"\"\n",
    "    if points.shape[1] < 3:\n",
    "        raise ValueError(\"Point cloud must have at least 3 columns (x, y, z).\")\n",
    "    color = []\n",
    "    for i in range(points.shape[0]):\n",
    "        color.append(np.hstack((points[i,3] / 255, points[i,4] /255, points[i,5] / 255)))  \n",
    "    fig = px.scatter_3d(\n",
    "        x=points[:, 0], y=points[:, 1], z=points[:, 2], color=color,\n",
    "        title=title,\n",
    "        opacity=0.8\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=s))\n",
    "    fig.update_layout(scene=dict(\n",
    "        xaxis_title='X', yaxis_title='Y', zaxis_title='Z'\n",
    "    ))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c30f53",
   "metadata": {},
   "source": [
    "# Tree-based methods and SVM\n",
    "\n",
    "For tree-based methods and support vector machines, a one-dimensional feature vector is needed as input data for every sample of the dataset. Therefore, in a first, basic approach, we simply downsampled each point cloud randomly to the number of points of the smallest point cloud in the dataset. Another option would be padding all samples of the dataset by adding dummy points to match the shape of the point cloud with the most points. Downsampling each point cloud would also be the main drawback of this approach as in some cases a huge amount of information about the point cloud gets lost (The smallest point cloud contains 510 points, while the largest one features 16661 points).\n",
    "\n",
    "Afterwards, we prepared the input feature matrix X by simply concatenating all selected features for all the points for each point cloud. Selecting different features could also give different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfdbee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Preparation for tree-based methods and SVM --- (Jonas)\n",
    "\n",
    "def downsample_point_clouds(dataset):\n",
    "    \"\"\"\n",
    "    Downsamples all point clouds in the dataset to the size of the first point cloud.\n",
    "\n",
    "    Args:\n",
    "        dataset (list of np.ndarray): List of point clouds.\n",
    "\n",
    "    Returns:\n",
    "        list of np.ndarray: List of downsampled point clouds.\n",
    "    \"\"\"\n",
    "    if not dataset:\n",
    "        return []\n",
    "\n",
    "    # Sort the dataset by the number of points in each point cloud\n",
    "    dataset.sort(key=lambda x: x.shape[0])\n",
    "    print(f\"Shape of smallest point cloud: {dataset[0].shape}\")  # Print the shape of the first point cloud\n",
    "    print(f\"Shape of largest point cloud: {dataset[-1].shape}\")  # Print the shape of the last point cloud\n",
    "\n",
    "    # Downsample all point clouds to the size of the first point cloud\n",
    "    for i in range(len(dataset)):\n",
    "        sample = dataset[i]  # Get the current sample\n",
    "        indices = np.random.choice(sample.shape[0], size=dataset[0].shape[0], replace=False)  # Randomly select indices\n",
    "        sample_downsampled = sample[indices]  # Downsample the point cloud to match the size of the first point cloud \n",
    "        dataset[i] = sample_downsampled  # Update the dataset with the downsampled sample\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def pad_point_clouds(dataset):\n",
    "    \"\"\"\n",
    "    Pads all point clouds in the dataset to the size of the largest point cloud.\n",
    "\n",
    "    Args:\n",
    "        dataset (list of np.ndarray): List of point clouds.\n",
    "\n",
    "    Returns:\n",
    "        list of np.ndarray: List of padded point clouds.\n",
    "    \"\"\"\n",
    "    if not dataset:\n",
    "        return []\n",
    "\n",
    "    max_size = max(sample.shape[0] for sample in dataset)  # Find the maximum size of point clouds\n",
    "    padded_dataset = []\n",
    "\n",
    "    for sample in dataset:\n",
    "        if sample.shape[0] < max_size:\n",
    "            padding = np.zeros((max_size - sample.shape[0], sample.shape[1]))  # Create padding\n",
    "            padded_sample = np.vstack((sample, padding))  # Stack the original sample with padding\n",
    "        else:\n",
    "            padded_sample = sample  # No padding needed\n",
    "        padded_dataset.append(padded_sample)  # Append the padded sample to the new dataset\n",
    "\n",
    "    return padded_dataset\n",
    "\n",
    "\n",
    "def prepare_feature_vectors(dataset):\n",
    "    \"\"\"\n",
    "    Prepares feature vectors and labels from the dataset of point clouds.\n",
    "\n",
    "    Args:\n",
    "        dataset (list of np.ndarray): List of downsampled point clouds.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Feature vectors (X) and labels (y).\n",
    "    \"\"\"\n",
    "    # Convert the list of point clouds to a single NumPy array\n",
    "    feature_vectors = []\n",
    "    target_labels = []  # Initialize a list to store labels\n",
    "    for sample in dataset:\n",
    "        feature_vector = np.array([])  # Initialize an empty array for the feature vector\n",
    "        # Iterate over each point in the sample and concatenate its features to the feature vector\n",
    "        for i in range(sample.shape[0]):\n",
    "            feature_vector = np.hstack((feature_vector, sample[i, :-1]))\n",
    "        target_labels.append(sample[i, -1])  \n",
    "        # Concatenate all features except the label\n",
    "        feature_vectors.append(feature_vector)  # Append the feature vector to the list\n",
    "        \n",
    "    return np.vstack(feature_vectors), np.hstack(target_labels)  # Return stacked feature vectors and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3e29c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1425 point clouds from 6 categories.\n",
      "Shape of smallest point cloud: (510, 23)\n",
      "Shape of largest point cloud: (16661, 23)\n",
      "Dataset shape: (1425, 11220), Labels shape: (1425,)\n"
     ]
    }
   ],
   "source": [
    "# Load all point clouds from each category\n",
    "labels = [\"2lanes\", \"3lanes\", \"crossing\", \"split4lanes\", \"split6lanes\", \"transition\"]\n",
    "folder_path = \"C:/Users/jonas/OneDrive/Dokumente/Studium/Master/SoSe 2025/Machine Learning for Civil Engineering/Group Project/4_Road_point_cloud_classification/dataset/\"  # Adjust the folder path as needed\n",
    "dataset = load_point_clouds(labels, folder_path)  # Adjust the folder path as needed\n",
    "print(f\"Loaded {len(dataset)} point clouds from {len(labels)} categories.\")\n",
    "\n",
    "# Optionally visualize the first point cloud from each category\n",
    "for i, label in enumerate(labels):\n",
    "    if dataset:\n",
    "        visualize_point_cloud(dataset[i], title=f\"Point Cloud - {label} - {dataset[i].shape[0]} points\")\n",
    "    else:\n",
    "        print(f\"No point clouds found for label: {label}\")\n",
    "\n",
    "# Downsample the point clouds to the size of the first point cloud\n",
    "dataset = downsample_point_clouds(dataset) \n",
    "# Prepare feature vectors and labels\n",
    "X, y = prepare_feature_vectors(dataset)\n",
    "# Print the shape of the dataset\n",
    "print(f\"Dataset shape: {X.shape}, Labels shape: {y.shape}\")\n",
    "# Perform a train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea4be251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Accuracy: 59.38375350140056%\n",
      "Cross-Validation results: Mean accuracy: 56.35087719298245%, Standard deviation: 1.290159741112293%\n"
     ]
    }
   ],
   "source": [
    "# --- Random Forest Classifier with Cross-Validation ---\n",
    "\n",
    "# Initialize the Random Forest Classifier with specified parameters\n",
    "randomforest = RandomForestClassifier(n_estimators = 300, criterion = \"gini\", max_depth=3, random_state=0)\n",
    "# Fit the model to the training data\n",
    "randomforest.fit(X_train, y_train)\n",
    "# Evaluate the model on the test data\n",
    "y_pred = randomforest.predict(X_test)\n",
    "# Calculate the accuracy of the model\n",
    "acc_rf = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Classifier Accuracy: {acc_rf * 100}%\")\n",
    "\n",
    "# Perform cross-validation (CV) with 3 folds\n",
    "# Initialize KFold with 3 splits, no random state, and shuffling enabled\n",
    "cv = KFold(n_splits=3, random_state=None, shuffle=True)\n",
    "# Use the cross_val_score function to perform CV on the training data \n",
    "scores_rf = cross_val_score(randomforest, X,  y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# print out the mean accuracy and standard deviation over the 3 folds\n",
    "cv_mean = np.mean(np.array(scores_rf))\n",
    "cv_std = np.std(np.array(scores_rf))\n",
    "print(f\"Cross-Validation results: Mean accuracy: {cv_mean * 100}%, Standard deviation: {cv_std * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "129ab73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning:\n",
      "\n",
      "The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Classifier Accuracy: 50.98039215686274%\n",
      "Cross-Validation results: Mean accuracy: 44.98245614035088%, Standard deviation: 0.694701399060467%\n"
     ]
    }
   ],
   "source": [
    "# --- AdaBoost Classifier ---\n",
    "\n",
    "# Initialize the AdaBoost Classifier with the Decision Tree as base estimator\n",
    "adaboost = AdaBoostClassifier(n_estimators = 100, algorithm = \"SAMME\")\n",
    "# Fit the model to the training data\n",
    "adaboost.fit(X_train, y_train)\n",
    "# Predict the test set\n",
    "y_pred = adaboost.predict(X_test)\n",
    "# Calculate the accuracy of the model\n",
    "acc_adaboost = accuracy_score(y_test, y_pred)\n",
    "print(f\"AdaBoost Classifier Accuracy: {acc_adaboost * 100}%\")\n",
    "\n",
    "# Perform cross-validation (CV) with 3 folds\n",
    "# Initialize KFold with 3 splits, no random state, and shuffling enabled\n",
    "cv = KFold(n_splits=3, random_state=None, shuffle=True)\n",
    "# Use the cross_val_score function to perform CV on the training data\n",
    "scores_adaboost = cross_val_score(adaboost, X,  y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# print out the mean accuracy and standard deviation over the 3 folds\n",
    "cv_mean = np.mean(np.array(scores_adaboost))\n",
    "cv_std = np.std(np.array(scores_adaboost))\n",
    "print(f\"Cross-Validation results: Mean accuracy: {cv_mean * 100}%, Standard deviation: {cv_std * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "389584d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Classifier (One-vs-One Strategy) Accuracy : 43.69747899159664 %\n",
      "\n",
      "\n",
      "Cross-Validation results: Mean accuracy: 39.578947368421055%, Standard deviation: 1.0313641022244975%\n"
     ]
    }
   ],
   "source": [
    "# --- Support Vector Machine Classifier with One-vs-One Strategy ---\n",
    "\n",
    "# Apporach 1 \n",
    "svc_ovo  = OneVsOneClassifier(SVC(C=1, kernel=\"rbf\"))\n",
    "# Approach 2\n",
    "#clf = SVC(C=0.8, kernel=\"sigmoid\", decision_function_shape= \"ovo\")\n",
    "# Fit the model\n",
    "svc_ovo.fit(X_train,y_train)\n",
    "# Predict the test set\n",
    "predictions = svc_ovo .predict(X_test)\n",
    "# Compute accuracy\n",
    "acc_svc = accuracy_score(y_test, predictions)\n",
    "print(f\"Support Vector Classifier (One-vs-One Strategy) Accuracy : {acc_svc * 100} %\\n\\n\")\n",
    "\n",
    "# Perform cross-validation (CV) with 3 folds\n",
    "# Initialize KFold with 3 splits, no random state, and shuffling enabled\n",
    "cv = KFold(n_splits=3, random_state=None, shuffle=True)\n",
    "# Use the cross_val_score function to perform CV on the training data \n",
    "scores_svc = cross_val_score(svc_ovo, X,  y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# print out the mean accuracy and standard deviation over the 3 folds\n",
    "cv_mean = np.mean(np.array(scores_svc))\n",
    "cv_std = np.std(np.array(scores_svc))\n",
    "print(f\"Cross-Validation results: Mean accuracy: {cv_mean * 100}%, Standard deviation: {cv_std * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7660f461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Model  Accuracy  Cross-Validation Mean  Cross-Validation Std\n",
      "0   Random Forest  0.593838               0.563509              0.012902\n",
      "1       Ada Boost  0.509804               0.449825              0.006947\n",
      "2  SVM One-vs-One  0.436975               0.395789              0.010314\n"
     ]
    }
   ],
   "source": [
    "# Create a table with the results\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'Ada Boost', 'SVM One-vs-One'],\n",
    "    'Accuracy': [acc_rf, acc_adaboost, acc_svc],\n",
    "    'Cross-Validation Mean': [np.mean(np.array(scores_rf)), np.mean(np.array(scores_adaboost)), np.mean(np.array(scores_svc))],\n",
    "    'Cross-Validation Std': [np.std(np.array(scores_rf)), np.std(np.array(scores_adaboost)), np.std(np.array(scores_svc))]\n",
    "})  \n",
    "# Display the results table\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed34e070",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "In our second approach we tried to classify our dataset by using a classic convolutional neural network (CNN). A big advantage of using a convolutional neural network for our classification problem over using a Decision Tree or Support Vector Machine is, that we don´t have to downsample our dataset for feeding the CNN. On the other hand, a classical CNN is only able to process XYZ-Coordinates and RGB-Values, so all other features of the point cloud will get lost. As the CNN demands standard RGB-images as input values, we first implemented a conversion of the given point clouds to 2d-images as input for the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fd22c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Preparation for CNN --- (Jonas)\n",
    "\n",
    "def prepare_dataset(dataset, img_size=(200, 200)):\n",
    "    # Prepare feature vectors and labels for convolutional neural networks\n",
    "    target_labels = [pc[:, -1] for pc in dataset]  # Extract labels from each point cloud\n",
    "    point_clouds = [pc[:, :-1] for pc in dataset]  # Remove the last column (labels) from each point cloud\n",
    "    #Convert the list of point clouds to image-like tensors\n",
    "    pc_images = []  # Initialize an empty list to store images\n",
    "    # Iterate over each point cloud and convert it to an image\n",
    "    for i in range(len(point_clouds)):\n",
    "        pc = point_clouds[i]\n",
    "        # Convert the point cloud to an image representation\n",
    "        pc_images.append(convert_pc_to_image(pc, img_size))\n",
    "        print(f\"Converted {i + 1}/{len(point_clouds)} point clouds to images.\")\n",
    "    # Store images in numpy array\n",
    "    X = np.array(pc_images)\n",
    "    y = np.hstack(target_labels)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def convert_pc_to_image(pc, img_size=(200, 200)):\n",
    "    \"\"\"\n",
    "    Converts a point cloud to a 2D image representation.\n",
    "    \n",
    "    Args:\n",
    "        pc (np.ndarray): Point cloud of shape (N, 3) or (N, >=3).\n",
    "        img_size (tuple): Size of the output image (height, width).\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: 2D image representation of the point cloud.\n",
    "    \"\"\"\n",
    "    if pc.shape[1] < 3:\n",
    "        raise ValueError(\"Point cloud must have at least 3 columns (x, y, z).\")\n",
    "    \n",
    "    # Normalize the point cloud to fit within the image size\n",
    "    pc_normalized = (pc[:, :2] - np.min(pc[:, :2], axis=0)) / (np.max(pc[:, :2], axis=0) - np.min(pc[:, :2], axis=0))\n",
    "    pc_normalized *= img_size[0]  # Scale to image size\n",
    "    \n",
    "    # Create an empty image\n",
    "    img = np.zeros(img_size)\n",
    "    height, width = img_size\n",
    "\n",
    "    x, y, z, color = pc_normalized[:, 0], pc_normalized[:, 1], pc[:, 2], pc[:, 3:6] if pc.shape[1] >= 6 else np.zeros((pc.shape[0], 3)) \n",
    "\n",
    "    # --- Interpolation of Point Cloud Data for Visualization --- (Jonas)\n",
    "\n",
    "    # sort x,y,z by z in ascending order so the highest z is plotted over the lowest z\n",
    "    zSort = z.argsort()\n",
    "    x, y, z, color = x[zSort], y[zSort], z[zSort], color[zSort]\n",
    "\n",
    "    # interpolation\n",
    "    # generate a grid where the interpolation will be calculated\n",
    "    X, Y = np.meshgrid(np.arange(width), np.arange(height))\n",
    "\n",
    "    R = griddata(np.vstack((x, y)).T, color[:, 0], (X, Y), method='cubic')\n",
    "    Rlinear= griddata(np.vstack((x, y)).T, color[:, 0], (X, Y), method='nearest')\n",
    "    G = griddata(np.vstack((x, y)).T, color[:, 1], (X, Y), method='cubic')\n",
    "    Glinear= griddata(np.vstack((x, y)).T, color[:, 1], (X, Y), method='nearest')\n",
    "    B = griddata(np.vstack((x, y)).T, color[:, 2], (X, Y), method='cubic')\n",
    "    Blinear= griddata(np.vstack((x, y)).T, color[:, 2], (X, Y), method='nearest')\n",
    "\n",
    "    #Fill empty values with nearest neighbor\n",
    "    R[np.isnan(R)] = Rlinear[np.isnan(R)]\n",
    "    G[np.isnan(G)] = Glinear[np.isnan(G)]\n",
    "    B[np.isnan(B)] = Blinear[np.isnan(B)]\n",
    "\n",
    "    # Normalize the color channels to [0, 1]\n",
    "    R = R - np.min(R)\n",
    "    G = G - np.min(G)\n",
    "    B = B - np.min(B)\n",
    "    # Ensure no negative values\n",
    "    R[R < 0] = 0\n",
    "    G[G < 0] = 0\n",
    "    B[B < 0] = 0\n",
    "\n",
    "    R = R/np.max(R)\n",
    "    G = G/np.max(G)\n",
    "    B = B/np.max(B)\n",
    "\n",
    "    interpolated = cv2.merge((R, G, B))\n",
    "\n",
    "    return interpolated\n",
    "\n",
    "def show_image(img, title=\"Point Cloud Image\"):\n",
    "    \"\"\"\n",
    "    Displays a 2D image representation of a point cloud.\n",
    "    \n",
    "    Args:\n",
    "        img (np.ndarray): 2D image representation of the point cloud.\n",
    "        title (str): Title of the image.\n",
    "    \"\"\"\n",
    "    plt.imshow(img)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb36689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Building a Convolutional Neural Network --- (Jonas)\n",
    "# def build_cnn(input_shape, num_classes):\n",
    "#     \"\"\"\n",
    "#     Builds a Convolutional Neural Network (CNN) model.\n",
    "\n",
    "#     Args:\n",
    "#         input_shape (tuple): Shape of the input data (channels, height, width).\n",
    "#         num_classes (int): Number of output classes.\n",
    "\n",
    "#     Returns:\n",
    "#         torch.nn.Module: The CNN model.\n",
    "#     \"\"\"\n",
    "#     class CNN(nn.Module):\n",
    "#         def __init__(self):\n",
    "#             super(CNN, self).__init__()\n",
    "#             self.conv1 = nn.Conv2d(in_channels=input_shape[0], out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "#             self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "#             self.fc1 = nn.Linear(64 * input_shape[1] * input_shape[2] // 4, 128)\n",
    "#             self.fc2 = nn.Linear(128, num_classes)\n",
    "#             self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#             self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "#         def forward(self, x):\n",
    "#             x = self.pool(F.relu(self.conv1(x)))\n",
    "#             x = self.pool(F.relu(self.conv2(x)))\n",
    "#             x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "#             x = F.relu(self.fc1(x))\n",
    "#             x = self.dropout(x)\n",
    "#             x = self.fc2(x)\n",
    "#             return x\n",
    "\n",
    "#     return CNN()\n",
    "\n",
    "\n",
    "#CNN Model Definition  -- (Jonas)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_shape[0], out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "    \n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "    \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(p=0.6)  # Increased dropout\n",
    "\n",
    "        # Dynamically compute flattened size\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, *input_shape)\n",
    "            dummy = self.pool(F.relu(self.bn1(self.conv1(dummy))))\n",
    "            dummy = self.pool(F.relu(self.bn2(self.conv2(dummy))))\n",
    "            flattened_size = dummy.view(1, -1).shape[1]\n",
    "\n",
    "        self.fc1 = nn.Linear(flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  \n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "        x = x.view(x.size(0), -1)             \n",
    "        x = F.relu(self.fc1(x))               \n",
    "        x = self.dropout(x)                   \n",
    "        x = self.fc2(x)                       \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83a8a9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training and Evaluation of the CNN model --- (Shino Daniel)\n",
    "    \n",
    "# Learning curve tracking\n",
    "def plot_learning_curves(train_losses, test_losses, train_accuracies, test_accuracies):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs, test_losses, label='Test Loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(epochs, test_accuracies, label='Test Accuracy')\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, num_epochs=10, learning_rate=0.001):\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    train_losses, test_losses = [], []\n",
    "    train_accuracies, test_accuracies = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  \n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()            # Clear previous gradients\n",
    "            outputs = model(inputs)          # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()                  # Backpropagation\n",
    "            optimizer.step()                 # Update weights\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_acc = correct / total\n",
    "        test_loss, test_acc = evaluate_model(model, test_loader, device, silent=True)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    plot_learning_curves(train_losses, test_losses, train_accuracies, test_accuracies)\n",
    "\n",
    "# Evaluation with confusion matrix\n",
    "def evaluate_model(model, test_loader, device, class_names=None, silent=False):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    if not silent and class_names:\n",
    "        cm = confusion_matrix(all_labels, all_preds, labels=range(len(class_names)))\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "        disp.plot(cmap=\"Blues\", xticks_rotation=45)\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "    return avg_loss, accuracy if silent else accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8bdefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 1/1425 point clouds to images.\n",
      "Converted 2/1425 point clouds to images.\n",
      "Converted 3/1425 point clouds to images.\n",
      "Converted 4/1425 point clouds to images.\n",
      "Converted 5/1425 point clouds to images.\n",
      "Converted 6/1425 point clouds to images.\n",
      "Converted 7/1425 point clouds to images.\n",
      "Converted 8/1425 point clouds to images.\n",
      "Converted 9/1425 point clouds to images.\n",
      "Converted 10/1425 point clouds to images.\n",
      "Converted 11/1425 point clouds to images.\n",
      "Converted 12/1425 point clouds to images.\n",
      "Converted 13/1425 point clouds to images.\n",
      "Converted 14/1425 point clouds to images.\n",
      "Converted 15/1425 point clouds to images.\n",
      "Converted 16/1425 point clouds to images.\n",
      "Converted 17/1425 point clouds to images.\n",
      "Converted 18/1425 point clouds to images.\n",
      "Converted 19/1425 point clouds to images.\n",
      "Converted 20/1425 point clouds to images.\n",
      "Converted 21/1425 point clouds to images.\n",
      "Converted 22/1425 point clouds to images.\n",
      "Converted 23/1425 point clouds to images.\n",
      "Converted 24/1425 point clouds to images.\n",
      "Converted 25/1425 point clouds to images.\n",
      "Converted 26/1425 point clouds to images.\n",
      "Converted 27/1425 point clouds to images.\n",
      "Converted 28/1425 point clouds to images.\n",
      "Converted 29/1425 point clouds to images.\n",
      "Converted 30/1425 point clouds to images.\n",
      "Converted 31/1425 point clouds to images.\n",
      "Converted 32/1425 point clouds to images.\n",
      "Converted 33/1425 point clouds to images.\n",
      "Converted 34/1425 point clouds to images.\n",
      "Converted 35/1425 point clouds to images.\n",
      "Converted 36/1425 point clouds to images.\n",
      "Converted 37/1425 point clouds to images.\n",
      "Converted 38/1425 point clouds to images.\n",
      "Converted 39/1425 point clouds to images.\n",
      "Converted 40/1425 point clouds to images.\n",
      "Converted 41/1425 point clouds to images.\n",
      "Converted 42/1425 point clouds to images.\n",
      "Converted 43/1425 point clouds to images.\n",
      "Converted 44/1425 point clouds to images.\n",
      "Converted 45/1425 point clouds to images.\n",
      "Converted 46/1425 point clouds to images.\n",
      "Converted 47/1425 point clouds to images.\n",
      "Converted 48/1425 point clouds to images.\n",
      "Converted 49/1425 point clouds to images.\n",
      "Converted 50/1425 point clouds to images.\n",
      "Converted 51/1425 point clouds to images.\n",
      "Converted 52/1425 point clouds to images.\n",
      "Converted 53/1425 point clouds to images.\n",
      "Converted 54/1425 point clouds to images.\n",
      "Converted 55/1425 point clouds to images.\n",
      "Converted 56/1425 point clouds to images.\n",
      "Converted 57/1425 point clouds to images.\n",
      "Converted 58/1425 point clouds to images.\n",
      "Converted 59/1425 point clouds to images.\n",
      "Converted 60/1425 point clouds to images.\n",
      "Converted 61/1425 point clouds to images.\n",
      "Converted 62/1425 point clouds to images.\n",
      "Converted 63/1425 point clouds to images.\n",
      "Converted 64/1425 point clouds to images.\n",
      "Converted 65/1425 point clouds to images.\n",
      "Converted 66/1425 point clouds to images.\n",
      "Converted 67/1425 point clouds to images.\n",
      "Converted 68/1425 point clouds to images.\n",
      "Converted 69/1425 point clouds to images.\n",
      "Converted 70/1425 point clouds to images.\n",
      "Converted 71/1425 point clouds to images.\n",
      "Converted 72/1425 point clouds to images.\n",
      "Converted 73/1425 point clouds to images.\n",
      "Converted 74/1425 point clouds to images.\n",
      "Converted 75/1425 point clouds to images.\n",
      "Converted 76/1425 point clouds to images.\n",
      "Converted 77/1425 point clouds to images.\n",
      "Converted 78/1425 point clouds to images.\n",
      "Converted 79/1425 point clouds to images.\n",
      "Converted 80/1425 point clouds to images.\n",
      "Converted 81/1425 point clouds to images.\n",
      "Converted 82/1425 point clouds to images.\n",
      "Converted 83/1425 point clouds to images.\n",
      "Converted 84/1425 point clouds to images.\n",
      "Converted 85/1425 point clouds to images.\n",
      "Converted 86/1425 point clouds to images.\n",
      "Converted 87/1425 point clouds to images.\n",
      "Converted 88/1425 point clouds to images.\n",
      "Converted 89/1425 point clouds to images.\n",
      "Converted 90/1425 point clouds to images.\n",
      "Converted 91/1425 point clouds to images.\n",
      "Converted 92/1425 point clouds to images.\n",
      "Converted 93/1425 point clouds to images.\n",
      "Converted 94/1425 point clouds to images.\n",
      "Converted 95/1425 point clouds to images.\n",
      "Converted 96/1425 point clouds to images.\n",
      "Converted 97/1425 point clouds to images.\n",
      "Converted 98/1425 point clouds to images.\n",
      "Converted 99/1425 point clouds to images.\n",
      "Converted 100/1425 point clouds to images.\n",
      "Converted 101/1425 point clouds to images.\n",
      "Converted 102/1425 point clouds to images.\n",
      "Converted 103/1425 point clouds to images.\n",
      "Converted 104/1425 point clouds to images.\n",
      "Converted 105/1425 point clouds to images.\n",
      "Converted 106/1425 point clouds to images.\n",
      "Converted 107/1425 point clouds to images.\n",
      "Converted 108/1425 point clouds to images.\n",
      "Converted 109/1425 point clouds to images.\n",
      "Converted 110/1425 point clouds to images.\n",
      "Converted 111/1425 point clouds to images.\n",
      "Converted 112/1425 point clouds to images.\n",
      "Converted 113/1425 point clouds to images.\n",
      "Converted 114/1425 point clouds to images.\n",
      "Converted 115/1425 point clouds to images.\n",
      "Converted 116/1425 point clouds to images.\n",
      "Converted 117/1425 point clouds to images.\n",
      "Converted 118/1425 point clouds to images.\n",
      "Converted 119/1425 point clouds to images.\n",
      "Converted 120/1425 point clouds to images.\n",
      "Converted 121/1425 point clouds to images.\n",
      "Converted 122/1425 point clouds to images.\n",
      "Converted 123/1425 point clouds to images.\n",
      "Converted 124/1425 point clouds to images.\n",
      "Converted 125/1425 point clouds to images.\n",
      "Converted 126/1425 point clouds to images.\n",
      "Converted 127/1425 point clouds to images.\n",
      "Converted 128/1425 point clouds to images.\n",
      "Converted 129/1425 point clouds to images.\n",
      "Converted 130/1425 point clouds to images.\n",
      "Converted 131/1425 point clouds to images.\n",
      "Converted 132/1425 point clouds to images.\n",
      "Converted 133/1425 point clouds to images.\n",
      "Converted 134/1425 point clouds to images.\n",
      "Converted 135/1425 point clouds to images.\n",
      "Converted 136/1425 point clouds to images.\n",
      "Converted 137/1425 point clouds to images.\n",
      "Converted 138/1425 point clouds to images.\n",
      "Converted 139/1425 point clouds to images.\n",
      "Converted 140/1425 point clouds to images.\n",
      "Converted 141/1425 point clouds to images.\n",
      "Converted 142/1425 point clouds to images.\n",
      "Converted 143/1425 point clouds to images.\n",
      "Converted 144/1425 point clouds to images.\n",
      "Converted 145/1425 point clouds to images.\n",
      "Converted 146/1425 point clouds to images.\n",
      "Converted 147/1425 point clouds to images.\n",
      "Converted 148/1425 point clouds to images.\n",
      "Converted 149/1425 point clouds to images.\n",
      "Converted 150/1425 point clouds to images.\n",
      "Converted 151/1425 point clouds to images.\n",
      "Converted 152/1425 point clouds to images.\n",
      "Converted 153/1425 point clouds to images.\n",
      "Converted 154/1425 point clouds to images.\n",
      "Converted 155/1425 point clouds to images.\n",
      "Converted 156/1425 point clouds to images.\n",
      "Converted 157/1425 point clouds to images.\n",
      "Converted 158/1425 point clouds to images.\n",
      "Converted 159/1425 point clouds to images.\n",
      "Converted 160/1425 point clouds to images.\n",
      "Converted 161/1425 point clouds to images.\n",
      "Converted 162/1425 point clouds to images.\n",
      "Converted 163/1425 point clouds to images.\n",
      "Converted 164/1425 point clouds to images.\n",
      "Converted 165/1425 point clouds to images.\n",
      "Converted 166/1425 point clouds to images.\n",
      "Converted 167/1425 point clouds to images.\n",
      "Converted 168/1425 point clouds to images.\n",
      "Converted 169/1425 point clouds to images.\n",
      "Converted 170/1425 point clouds to images.\n",
      "Converted 171/1425 point clouds to images.\n",
      "Converted 172/1425 point clouds to images.\n",
      "Converted 173/1425 point clouds to images.\n",
      "Converted 174/1425 point clouds to images.\n",
      "Converted 175/1425 point clouds to images.\n",
      "Converted 176/1425 point clouds to images.\n",
      "Converted 177/1425 point clouds to images.\n",
      "Converted 178/1425 point clouds to images.\n",
      "Converted 179/1425 point clouds to images.\n",
      "Converted 180/1425 point clouds to images.\n",
      "Converted 181/1425 point clouds to images.\n",
      "Converted 182/1425 point clouds to images.\n",
      "Converted 183/1425 point clouds to images.\n",
      "Converted 184/1425 point clouds to images.\n",
      "Converted 185/1425 point clouds to images.\n",
      "Converted 186/1425 point clouds to images.\n",
      "Converted 187/1425 point clouds to images.\n",
      "Converted 188/1425 point clouds to images.\n",
      "Converted 189/1425 point clouds to images.\n",
      "Converted 190/1425 point clouds to images.\n",
      "Converted 191/1425 point clouds to images.\n",
      "Converted 192/1425 point clouds to images.\n",
      "Converted 193/1425 point clouds to images.\n",
      "Converted 194/1425 point clouds to images.\n",
      "Converted 195/1425 point clouds to images.\n",
      "Converted 196/1425 point clouds to images.\n",
      "Converted 197/1425 point clouds to images.\n",
      "Converted 198/1425 point clouds to images.\n",
      "Converted 199/1425 point clouds to images.\n",
      "Converted 200/1425 point clouds to images.\n",
      "Converted 201/1425 point clouds to images.\n",
      "Converted 202/1425 point clouds to images.\n",
      "Converted 203/1425 point clouds to images.\n",
      "Converted 204/1425 point clouds to images.\n",
      "Converted 205/1425 point clouds to images.\n",
      "Converted 206/1425 point clouds to images.\n",
      "Converted 207/1425 point clouds to images.\n",
      "Converted 208/1425 point clouds to images.\n",
      "Converted 209/1425 point clouds to images.\n",
      "Converted 210/1425 point clouds to images.\n",
      "Converted 211/1425 point clouds to images.\n",
      "Converted 212/1425 point clouds to images.\n",
      "Converted 213/1425 point clouds to images.\n",
      "Converted 214/1425 point clouds to images.\n",
      "Converted 215/1425 point clouds to images.\n",
      "Converted 216/1425 point clouds to images.\n",
      "Converted 217/1425 point clouds to images.\n",
      "Converted 218/1425 point clouds to images.\n",
      "Converted 219/1425 point clouds to images.\n",
      "Converted 220/1425 point clouds to images.\n",
      "Converted 221/1425 point clouds to images.\n",
      "Converted 222/1425 point clouds to images.\n",
      "Converted 223/1425 point clouds to images.\n",
      "Converted 224/1425 point clouds to images.\n",
      "Converted 225/1425 point clouds to images.\n",
      "Converted 226/1425 point clouds to images.\n",
      "Converted 227/1425 point clouds to images.\n",
      "Converted 228/1425 point clouds to images.\n",
      "Converted 229/1425 point clouds to images.\n",
      "Converted 230/1425 point clouds to images.\n",
      "Converted 231/1425 point clouds to images.\n",
      "Converted 232/1425 point clouds to images.\n",
      "Converted 233/1425 point clouds to images.\n",
      "Converted 234/1425 point clouds to images.\n",
      "Converted 235/1425 point clouds to images.\n",
      "Converted 236/1425 point clouds to images.\n",
      "Converted 237/1425 point clouds to images.\n",
      "Converted 238/1425 point clouds to images.\n",
      "Converted 239/1425 point clouds to images.\n",
      "Converted 240/1425 point clouds to images.\n",
      "Converted 241/1425 point clouds to images.\n",
      "Converted 242/1425 point clouds to images.\n",
      "Converted 243/1425 point clouds to images.\n",
      "Converted 244/1425 point clouds to images.\n",
      "Converted 245/1425 point clouds to images.\n",
      "Converted 246/1425 point clouds to images.\n",
      "Converted 247/1425 point clouds to images.\n",
      "Converted 248/1425 point clouds to images.\n",
      "Converted 249/1425 point clouds to images.\n",
      "Converted 250/1425 point clouds to images.\n",
      "Converted 251/1425 point clouds to images.\n",
      "Converted 252/1425 point clouds to images.\n",
      "Converted 253/1425 point clouds to images.\n",
      "Converted 254/1425 point clouds to images.\n",
      "Converted 255/1425 point clouds to images.\n",
      "Converted 256/1425 point clouds to images.\n",
      "Converted 257/1425 point clouds to images.\n",
      "Converted 258/1425 point clouds to images.\n",
      "Converted 259/1425 point clouds to images.\n",
      "Converted 260/1425 point clouds to images.\n",
      "Converted 261/1425 point clouds to images.\n",
      "Converted 262/1425 point clouds to images.\n",
      "Converted 263/1425 point clouds to images.\n",
      "Converted 264/1425 point clouds to images.\n",
      "Converted 265/1425 point clouds to images.\n",
      "Converted 266/1425 point clouds to images.\n",
      "Converted 267/1425 point clouds to images.\n",
      "Converted 268/1425 point clouds to images.\n",
      "Converted 269/1425 point clouds to images.\n",
      "Converted 270/1425 point clouds to images.\n",
      "Converted 271/1425 point clouds to images.\n",
      "Converted 272/1425 point clouds to images.\n",
      "Converted 273/1425 point clouds to images.\n",
      "Converted 274/1425 point clouds to images.\n",
      "Converted 275/1425 point clouds to images.\n",
      "Converted 276/1425 point clouds to images.\n",
      "Converted 277/1425 point clouds to images.\n",
      "Converted 278/1425 point clouds to images.\n",
      "Converted 279/1425 point clouds to images.\n",
      "Converted 280/1425 point clouds to images.\n",
      "Converted 281/1425 point clouds to images.\n",
      "Converted 282/1425 point clouds to images.\n",
      "Converted 283/1425 point clouds to images.\n",
      "Converted 284/1425 point clouds to images.\n",
      "Converted 285/1425 point clouds to images.\n",
      "Converted 286/1425 point clouds to images.\n",
      "Converted 287/1425 point clouds to images.\n",
      "Converted 288/1425 point clouds to images.\n",
      "Converted 289/1425 point clouds to images.\n",
      "Converted 290/1425 point clouds to images.\n",
      "Converted 291/1425 point clouds to images.\n"
     ]
    }
   ],
   "source": [
    "labels = [\"2lanes\", \"3lanes\", \"crossing\", \"split4lanes\", \"split6lanes\", \"transition\"]\n",
    "# folder_path = \"C:/Users/jonas/OneDrive/Dokumente/Studium/Master/SoSe 2025/Machine Learning for Civil Engineering/Group Project/4_Road_point_cloud_classification/dataset/\"  # Adjust the folder path as needed\n",
    "dataset = load_point_clouds(labels)  # Load point clouds from the specified folder\n",
    "img_size = (400, 400)  # Define the size of the output images\n",
    "# Prepare the dataset for deep learning\n",
    "X, y = prepare_dataset(dataset, img_size=img_size)\n",
    "# Save the prepared dataset to a file\n",
    "np.savez(\"prepared_dataset.npz\", X=X, y=y)\n",
    "\n",
    "# Perform a train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "# Optionally visualize the first point cloud from each category as an image\n",
    "for i, label in enumerate(labels):\n",
    "    show_image(convert_pc_to_image(X_train[i]), title=f\"Point Cloud - {label} - {X_train[i].shape[0]} points\")\n",
    "\n",
    "# Optionally visualize the first point cloud from each category as an image\n",
    "for i, label in enumerate(labels):\n",
    "    for pc in dataset:\n",
    "        # Extract the label from the last column of the point cloud\n",
    "        pc_label = int(pc[0, -1])\n",
    "        if pc_label == i:\n",
    "            visualize_point_cloud(pc[:, :3], title=f\"Point Cloud - {label} - {pc.shape[0]} points\")\n",
    "            break\n",
    "        \n",
    "# Build the CNN model\n",
    "input_shape = (3, img_size)  # Assuming RGB images of size\n",
    "\n",
    "\n",
    "# Convert images and labels to torch tensors and reshape for CNN input\n",
    "# Assume X, y are numpy arrays from prepare_dataset(), X.shape = (N, H, W, 3)\n",
    "X = np.transpose(X, (0, 3, 1, 2))  # (N, 3, H, W)\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)  #changed it to float32 to avoid memory loss issues\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42, stratify=y_tensor)\n",
    "\n",
    "# Wrap in DataLoader\n",
    "batch_size = 16\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define input shape and number of classes for the CNN\n",
    "input_shape = (3, X.shape[2], X.shape[3])  # (C, H, W)\n",
    "num_classes = len(np.unique(y))\n",
    "model = CNN(input_shape, num_classes)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, test_loader, num_epochs=15, learning_rate=0.001)\n",
    "evaluate_model(model, test_loader, torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), class_names=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ce190f",
   "metadata": {},
   "source": [
    "# RandLANet\n",
    "Finally, after some literature research, we came up with the idea to classify our dataset by using an adapted version of the semantic segmentation model RandLANet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
