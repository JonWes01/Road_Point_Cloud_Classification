{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c7fc1a",
   "metadata": {},
   "source": [
    "# Road Point Cloud Classification\n",
    "1. Data Loader\n",
    "2. Tree-based methods and SVM\n",
    "3. Convolutional Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d982e21",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.13.5)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/jonas/OneDrive/Dokumente/Studium/Master/SoSe 2025/Machine Learning for Civil Engineering/Group Project/4_Road_point_cloud_classification/Road-Point-Cloud-Classification/.venv/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#install required packages\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41ea1e0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Basic data processing & preparation functionality\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minterpolate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m griddata\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Basic data processing & preparation functionality\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import griddata\n",
    "import cv2\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.colors as pc\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import open3d as o3d\n",
    "\n",
    "# Measurements of time to fit a model\n",
    "import time\n",
    "\n",
    "#   Tree-based methods\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#   SVM related methods\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Standard test dataset\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dataset tuning\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Model tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Metric for model evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Deep learning methods\n",
    "from tqdm import tqdm\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "#from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "105f894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Road Point Cloud Classification - Data Loader --- (Jonas)\n",
    "\n",
    "# Function to load all point clouds from a folder\n",
    "def load_point_clouds(labels, folder_path=\"dataset/\"):\n",
    "    \"\"\"\n",
    "    Loads all .npy point cloud files from the specified folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing .npy files.\n",
    "\n",
    "    Returns:\n",
    "        list of np.ndarray: List of loaded point cloud arrays.\n",
    "    \"\"\"\n",
    "    point_clouds = []\n",
    "    for label in labels:\n",
    "        folder = f\"{folder_path}{label}\"  # Adjust the folder paths as needed\n",
    "        for filename in os.listdir(folder):\n",
    "            if filename.endswith('.npy'):\n",
    "                file_path = os.path.join(folder, filename)\n",
    "                data = np.load(file_path)\n",
    "                # print(data.shape)  # Print shape of each loaded point cloud\n",
    "                label_vector = np.zeros((data.shape[0], 1))  # Create a label vector of zeros\n",
    "                label_vector.fill(labels.index(label))  # Fill the label vector with the index of the label\n",
    "                data = np.hstack((data, label_vector))  # Concatenate label vector to the point cloud data\n",
    "                # print(data.shape)  # Print shape after concatenation\n",
    "                point_clouds.append(data)\n",
    "    return point_clouds\n",
    "\n",
    "def visualize_point_cloud(points, title=\"Point Cloud\", color='b', s=1):\n",
    "    \"\"\"\n",
    "    Visualizes a 3D point cloud using open3d.\n",
    "\n",
    "    Args:\n",
    "        points (np.ndarray): Array of shape (N, 3) or (N, >=3) with x, y, z coordinates.\n",
    "        title (str): Plot title.\n",
    "        color (str or array): Color of points.\n",
    "        s (float): Marker size.\n",
    "    \"\"\"\n",
    "    if points.shape[1] < 3:\n",
    "        raise ValueError(\"Point cloud must have at least 3 columns (x, y, z).\")\n",
    "    # Create an Open3D point cloud object\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    # Set the points\n",
    "    pcd.points = o3d.utility.Vector3dVector(points[:, :3])  # Use only the first three columns for x, y, z\n",
    "    # Set the colors if available\n",
    "    if points.shape[1] >= 6:\n",
    "        colors = points[:, 3:6] / 255.0  # Normalize RGB values to [0, 1]\n",
    "        pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    else:\n",
    "        pcd.paint_uniform_color(pc.hex_to_rgb(color))  # Use a single color if no RGB columns are present\n",
    "    # Visualize the point cloud\n",
    "    o3d.visualization.draw_geometries([pcd], window_name=title, width=800, height=600, point_show_normal=False)\n",
    "    \n",
    "\n",
    "\n",
    "def visualize_point_cloud_plotly(points, title=\"Point Cloud\", s=2):\n",
    "    \"\"\"\n",
    "    Visualizes a 3D point cloud using plotly.\n",
    "\n",
    "    Args:\n",
    "        points (np.ndarray): Array of shape (N, 3) or (N, >=3) with x, y, z coordinates.\n",
    "        title (str): Plot title.\n",
    "        s (float): Marker size.\n",
    "    \"\"\"\n",
    "    if points.shape[1] < 3:\n",
    "        raise ValueError(\"Point cloud must have at least 3 columns (x, y, z).\")\n",
    "    color = []\n",
    "    for i in range(points.shape[0]):\n",
    "        color.append(np.hstack((points[i,3] / 255, points[i,4] /255, points[i,5] / 255)))  \n",
    "    fig = px.scatter_3d(\n",
    "        x=points[:, 0], y=points[:, 1], z=points[:, 2], color=color,\n",
    "        title=title,\n",
    "        opacity=0.8\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=s))\n",
    "    fig.update_layout(scene=dict(\n",
    "        xaxis_title='X', yaxis_title='Y', zaxis_title='Z'\n",
    "    ))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c30f53",
   "metadata": {},
   "source": [
    "# Tree-based methods and SVM\n",
    "\n",
    "For tree-based methods and support vector machines, a one-dimensional feature vector is needed as input data for every sample of the dataset. Therefore, in a first, basic approach, we simply downsampled each point cloud randomly to the number of points of the smallest point cloud in the dataset. Another option would be padding all samples of the dataset by adding dummy points to match the shape of the point cloud with the most points. Downsampling each point cloud would also be the main drawback of this approach as in some cases a huge amount of information about the point cloud gets lost (The smallest point cloud contains 510 points, while the largest one features 16661 points).\n",
    "\n",
    "Afterwards, we prepared the input feature matrix X by simply concatenating all selected features for all the points for each point cloud. Selecting different features could also give different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfdbee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Preparation for tree-based methods and SVM --- (Jonas)\n",
    "\n",
    "def downsample_point_clouds(dataset):\n",
    "    \"\"\"\n",
    "    Downsamples all point clouds in the dataset to the size of the first point cloud.\n",
    "\n",
    "    Args:\n",
    "        dataset (list of np.ndarray): List of point clouds.\n",
    "\n",
    "    Returns:\n",
    "        list of np.ndarray: List of downsampled point clouds.\n",
    "    \"\"\"\n",
    "    if not dataset:\n",
    "        return []\n",
    "\n",
    "    # Sort the dataset by the number of points in each point cloud\n",
    "    dataset.sort(key=lambda x: x.shape[0])\n",
    "    print(f\"Shape of smallest point cloud: {dataset[0].shape}\")  # Print the shape of the first point cloud\n",
    "    print(f\"Shape of largest point cloud: {dataset[-1].shape}\")  # Print the shape of the last point cloud\n",
    "\n",
    "    # Downsample all point clouds to the size of the first point cloud\n",
    "    for i in range(len(dataset)):\n",
    "        sample = dataset[i]  # Get the current sample\n",
    "        indices = np.random.choice(sample.shape[0], size=dataset[0].shape[0], replace=False)  # Randomly select indices\n",
    "        sample_downsampled = sample[indices]  # Downsample the point cloud to match the size of the first point cloud \n",
    "        dataset[i] = sample_downsampled  # Update the dataset with the downsampled sample\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def pad_point_clouds(dataset):\n",
    "    \"\"\"\n",
    "    Pads all point clouds in the dataset to the size of the largest point cloud.\n",
    "\n",
    "    Args:\n",
    "        dataset (list of np.ndarray): List of point clouds.\n",
    "\n",
    "    Returns:\n",
    "        list of np.ndarray: List of padded point clouds.\n",
    "    \"\"\"\n",
    "    if not dataset:\n",
    "        return []\n",
    "\n",
    "    max_size = max(sample.shape[0] for sample in dataset)  # Find the maximum size of point clouds\n",
    "    padded_dataset = []\n",
    "\n",
    "    for sample in dataset:\n",
    "        if sample.shape[0] < max_size:\n",
    "            padding = np.zeros((max_size - sample.shape[0], sample.shape[1]))  # Create padding\n",
    "            padded_sample = np.vstack((sample, padding))  # Stack the original sample with padding\n",
    "        else:\n",
    "            padded_sample = sample  # No padding needed\n",
    "        padded_dataset.append(padded_sample)  # Append the padded sample to the new dataset\n",
    "\n",
    "    return padded_dataset\n",
    "\n",
    "\n",
    "def prepare_feature_vectors(dataset):\n",
    "    \"\"\"\n",
    "    Prepares feature vectors and labels from the dataset of point clouds.\n",
    "\n",
    "    Args:\n",
    "        dataset (list of np.ndarray): List of downsampled point clouds.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Feature vectors (X) and labels (y).\n",
    "    \"\"\"\n",
    "    # Convert the list of point clouds to a single NumPy array\n",
    "    feature_vectors = []\n",
    "    target_labels = []  # Initialize a list to store labels\n",
    "    for sample in dataset:\n",
    "        feature_vector = np.array([])  # Initialize an empty array for the feature vector\n",
    "        # Iterate over each point in the sample and concatenate its features to the feature vector\n",
    "        for i in range(sample.shape[0]):\n",
    "            feature_vector = np.hstack((feature_vector, sample[i, :-1]))\n",
    "        target_labels.append(sample[i, -1])  \n",
    "        # Concatenate all features except the label\n",
    "        feature_vectors.append(feature_vector)  # Append the feature vector to the list\n",
    "        \n",
    "    return np.vstack(feature_vectors), np.hstack(target_labels)  # Return stacked feature vectors and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3e29c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1425 point clouds from 6 categories.\n",
      "Shape of smallest point cloud: (510, 23)\n",
      "Shape of largest point cloud: (16661, 23)\n",
      "Dataset shape: (1425, 11220), Labels shape: (1425,)\n"
     ]
    }
   ],
   "source": [
    "# Load all point clouds from each category\n",
    "labels = [\"2lanes\", \"3lanes\", \"crossing\", \"split4lanes\", \"split6lanes\", \"transition\"]\n",
    "folder_path = \"C:/Users/jonas/OneDrive/Dokumente/Studium/Master/SoSe 2025/Machine Learning for Civil Engineering/Group Project/4_Road_point_cloud_classification/dataset/\"  # Adjust the folder path as needed\n",
    "dataset = load_point_clouds(labels, folder_path)  # Adjust the folder path as needed\n",
    "print(f\"Loaded {len(dataset)} point clouds from {len(labels)} categories.\")\n",
    "\n",
    "# Optionally visualize the first point cloud from each category\n",
    "for i, label in enumerate(labels):\n",
    "    if dataset:\n",
    "        visualize_point_cloud(dataset[i], title=f\"Point Cloud - {label} - {dataset[i].shape[0]} points\")\n",
    "    else:\n",
    "        print(f\"No point clouds found for label: {label}\")\n",
    "\n",
    "# Downsample the point clouds to the size of the first point cloud\n",
    "dataset = downsample_point_clouds(dataset) \n",
    "# Prepare feature vectors and labels\n",
    "X, y = prepare_feature_vectors(dataset)\n",
    "# Print the shape of the dataset\n",
    "print(f\"Dataset shape: {X.shape}, Labels shape: {y.shape}\")\n",
    "# Perform a train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea4be251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Accuracy: 59.38375350140056%\n",
      "Cross-Validation results: Mean accuracy: 56.35087719298245%, Standard deviation: 1.290159741112293%\n"
     ]
    }
   ],
   "source": [
    "# --- Random Forest Classifier with Cross-Validation ---\n",
    "\n",
    "# Initialize the Random Forest Classifier with specified parameters\n",
    "randomforest = RandomForestClassifier(n_estimators = 300, criterion = \"gini\", max_depth=3, random_state=0)\n",
    "# Fit the model to the training data\n",
    "randomforest.fit(X_train, y_train)\n",
    "# Evaluate the model on the test data\n",
    "y_pred = randomforest.predict(X_test)\n",
    "# Calculate the accuracy of the model\n",
    "acc_rf = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Classifier Accuracy: {acc_rf * 100}%\")\n",
    "\n",
    "# Perform cross-validation (CV) with 3 folds\n",
    "# Initialize KFold with 3 splits, no random state, and shuffling enabled\n",
    "cv = KFold(n_splits=3, random_state=None, shuffle=True)\n",
    "# Use the cross_val_score function to perform CV on the training data \n",
    "scores_rf = cross_val_score(randomforest, X,  y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# print out the mean accuracy and standard deviation over the 3 folds\n",
    "cv_mean = np.mean(np.array(scores_rf))\n",
    "cv_std = np.std(np.array(scores_rf))\n",
    "print(f\"Cross-Validation results: Mean accuracy: {cv_mean * 100}%, Standard deviation: {cv_std * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "129ab73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning:\n",
      "\n",
      "The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Classifier Accuracy: 50.98039215686274%\n",
      "Cross-Validation results: Mean accuracy: 44.98245614035088%, Standard deviation: 0.694701399060467%\n"
     ]
    }
   ],
   "source": [
    "# --- AdaBoost Classifier ---\n",
    "\n",
    "# Initialize the AdaBoost Classifier with the Decision Tree as base estimator\n",
    "adaboost = AdaBoostClassifier(n_estimators = 100, algorithm = \"SAMME\")\n",
    "# Fit the model to the training data\n",
    "adaboost.fit(X_train, y_train)\n",
    "# Predict the test set\n",
    "y_pred = adaboost.predict(X_test)\n",
    "# Calculate the accuracy of the model\n",
    "acc_adaboost = accuracy_score(y_test, y_pred)\n",
    "print(f\"AdaBoost Classifier Accuracy: {acc_adaboost * 100}%\")\n",
    "\n",
    "# Perform cross-validation (CV) with 3 folds\n",
    "# Initialize KFold with 3 splits, no random state, and shuffling enabled\n",
    "cv = KFold(n_splits=3, random_state=None, shuffle=True)\n",
    "# Use the cross_val_score function to perform CV on the training data\n",
    "scores_adaboost = cross_val_score(adaboost, X,  y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# print out the mean accuracy and standard deviation over the 3 folds\n",
    "cv_mean = np.mean(np.array(scores_adaboost))\n",
    "cv_std = np.std(np.array(scores_adaboost))\n",
    "print(f\"Cross-Validation results: Mean accuracy: {cv_mean * 100}%, Standard deviation: {cv_std * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "389584d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Classifier (One-vs-One Strategy) Accuracy : 43.69747899159664 %\n",
      "\n",
      "\n",
      "Cross-Validation results: Mean accuracy: 39.578947368421055%, Standard deviation: 1.0313641022244975%\n"
     ]
    }
   ],
   "source": [
    "# --- Support Vector Machine Classifier with One-vs-One Strategy ---\n",
    "\n",
    "# Apporach 1 \n",
    "svc_ovo  = OneVsOneClassifier(SVC(C=1, kernel=\"rbf\"))\n",
    "# Approach 2\n",
    "#clf = SVC(C=0.8, kernel=\"sigmoid\", decision_function_shape= \"ovo\")\n",
    "# Fit the model\n",
    "svc_ovo.fit(X_train,y_train)\n",
    "# Predict the test set\n",
    "predictions = svc_ovo .predict(X_test)\n",
    "# Compute accuracy\n",
    "acc_svc = accuracy_score(y_test, predictions)\n",
    "print(f\"Support Vector Classifier (One-vs-One Strategy) Accuracy : {acc_svc * 100} %\\n\\n\")\n",
    "\n",
    "# Perform cross-validation (CV) with 3 folds\n",
    "# Initialize KFold with 3 splits, no random state, and shuffling enabled\n",
    "cv = KFold(n_splits=3, random_state=None, shuffle=True)\n",
    "# Use the cross_val_score function to perform CV on the training data \n",
    "scores_svc = cross_val_score(svc_ovo, X,  y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# print out the mean accuracy and standard deviation over the 3 folds\n",
    "cv_mean = np.mean(np.array(scores_svc))\n",
    "cv_std = np.std(np.array(scores_svc))\n",
    "print(f\"Cross-Validation results: Mean accuracy: {cv_mean * 100}%, Standard deviation: {cv_std * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7660f461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Model  Accuracy  Cross-Validation Mean  Cross-Validation Std\n",
      "0   Random Forest  0.593838               0.563509              0.012902\n",
      "1       Ada Boost  0.509804               0.449825              0.006947\n",
      "2  SVM One-vs-One  0.436975               0.395789              0.010314\n"
     ]
    }
   ],
   "source": [
    "# Create a table with the results\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'Ada Boost', 'SVM One-vs-One'],\n",
    "    'Accuracy': [acc_rf, acc_adaboost, acc_svc],\n",
    "    'Cross-Validation Mean': [np.mean(np.array(scores_rf)), np.mean(np.array(scores_adaboost)), np.mean(np.array(scores_svc))],\n",
    "    'Cross-Validation Std': [np.std(np.array(scores_rf)), np.std(np.array(scores_adaboost)), np.std(np.array(scores_svc))]\n",
    "})  \n",
    "# Display the results table\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed34e070",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "In our second approach we tried to classify our dataset by using a classic convolutional neural network (CNN). A big advantage of using a convolutional neural network for our classification problem over using a Decision Tree or Support Vector Machine is, that we don´t have to downsample our dataset for feeding the CNN. On the other hand, a classical CNN is only able to process XYZ-Coordinates and RGB-Values, so all other features of the point cloud will get lost. As the CNN demands standard RGB-images as input values, we first implemented a conversion of the given point clouds to 2d-images as input for the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fd22c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Preparation for CNN --- (Jonas)\n",
    "\n",
    "def prepare_dataset(dataset, img_size=(200, 200)):\n",
    "    # Prepare feature vectors and labels for convolutional neural networks\n",
    "    target_labels = [pc[:, -1] for pc in dataset]  # Extract labels from each point cloud\n",
    "    point_clouds = [pc[:, :-1] for pc in dataset]  # Remove the last column (labels) from each point cloud\n",
    "    #Convert the list of point clouds to image-like tensors\n",
    "    pc_images = []  # Initialize an empty list to store images\n",
    "    # Iterate over each point cloud and convert it to an image\n",
    "    for pc in point_clouds:\n",
    "        pc_images.append(convert_pc_to_image(pc, img_size))\n",
    "        print(f\"Converted {point_clouds.index(pc) + 1}/{len(point_clouds)} point clouds to images.\")\n",
    "    # Store images in numpy array\n",
    "    X = np.array(pc_images)\n",
    "    y = np.hstack(target_labels)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def convert_pc_to_image(pc, img_size=(200, 200)):\n",
    "    \"\"\"\n",
    "    Converts a point cloud to a 2D image representation.\n",
    "    \n",
    "    Args:\n",
    "        pc (np.ndarray): Point cloud of shape (N, 3) or (N, >=3).\n",
    "        img_size (tuple): Size of the output image (height, width).\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: 2D image representation of the point cloud.\n",
    "    \"\"\"\n",
    "    if pc.shape[1] < 3:\n",
    "        raise ValueError(\"Point cloud must have at least 3 columns (x, y, z).\")\n",
    "    \n",
    "    # Normalize the point cloud to fit within the image size\n",
    "    pc_normalized = (pc[:, :2] - np.min(pc[:, :2], axis=0)) / (np.max(pc[:, :2], axis=0) - np.min(pc[:, :2], axis=0))\n",
    "    pc_normalized *= img_size[0]  # Scale to image size\n",
    "    \n",
    "    # Create an empty image\n",
    "    img = np.zeros(img_size)\n",
    "    height, width = img_size\n",
    "\n",
    "    x, y, z, color = pc_normalized[:, 0], pc_normalized[:, 1], pc[:, 2], pc[:, 3:6] if pc.shape[1] >= 6 else np.zeros((pc.shape[0], 3)) \n",
    "\n",
    "    # --- Interpolation of Point Cloud Data for Visualization --- (Jonas)\n",
    "\n",
    "    # sort x,y,z by z in ascending order so the highest z is plotted over the lowest z\n",
    "    zSort = z.argsort()\n",
    "    x, y, z, color = x[zSort], y[zSort], z[zSort], color[zSort]\n",
    "\n",
    "    # interpolation\n",
    "    # generate a grid where the interpolation will be calculated\n",
    "    X, Y = np.meshgrid(np.arange(width), np.arange(height))\n",
    "\n",
    "    R = griddata(np.vstack((x, y)).T, color[:, 0], (X, Y), method='cubic')\n",
    "    Rlinear= griddata(np.vstack((x, y)).T, color[:, 0], (X, Y), method='nearest')\n",
    "    G = griddata(np.vstack((x, y)).T, color[:, 1], (X, Y), method='cubic')\n",
    "    Glinear= griddata(np.vstack((x, y)).T, color[:, 1], (X, Y), method='nearest')\n",
    "    B = griddata(np.vstack((x, y)).T, color[:, 2], (X, Y), method='cubic')\n",
    "    Blinear= griddata(np.vstack((x, y)).T, color[:, 2], (X, Y), method='nearest')\n",
    "\n",
    "    #Fill empty values with nearest neighbor\n",
    "    R[np.isnan(R)] = Rlinear[np.isnan(R)]\n",
    "    G[np.isnan(G)] = Glinear[np.isnan(G)]\n",
    "    B[np.isnan(B)] = Blinear[np.isnan(B)]\n",
    "\n",
    "    # Normalize the color channels to [0, 1]\n",
    "    R = R - np.min(R)\n",
    "    G = G - np.min(G)\n",
    "    B = B - np.min(B)\n",
    "    # Ensure no negative values\n",
    "    R[R < 0] = 0\n",
    "    G[G < 0] = 0\n",
    "    B[B < 0] = 0\n",
    "\n",
    "    R = R/np.max(R)\n",
    "    G = G/np.max(G)\n",
    "    B = B/np.max(B)\n",
    "\n",
    "    interpolated = cv2.merge((R, G, B))\n",
    "\n",
    "    return interpolated\n",
    "\n",
    "def show_image(img, title=\"Point Cloud Image\"):\n",
    "    \"\"\"\n",
    "    Displays a 2D image representation of a point cloud.\n",
    "    \n",
    "    Args:\n",
    "        img (np.ndarray): 2D image representation of the point cloud.\n",
    "        title (str): Title of the image.\n",
    "    \"\"\"\n",
    "    plt.imshow(img)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bb36689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Building a Convolutional Neural Network --- (Jonas)\n",
    "def build_cnn(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Builds a Convolutional Neural Network (CNN) model.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Shape of the input data (channels, height, width).\n",
    "        num_classes (int): Number of output classes.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The CNN model.\n",
    "    \"\"\"\n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNN, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(in_channels=input_shape[0], out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "            self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "            self.fc1 = nn.Linear(64 * input_shape[1] * input_shape[2] // 4, 128)\n",
    "            self.fc2 = nn.Linear(128, num_classes)\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.pool(F.relu(self.conv1(x)))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "            x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "    return CNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b8bdefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 1/1425 point clouds to images.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1512,22) (1533,22) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m img_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m400\u001b[39m, \u001b[38;5;241m400\u001b[39m)  \u001b[38;5;66;03m# Define the size of the output images\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Prepare the dataset for deep learning\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Perform a train-test-split\u001b[39;00m\n\u001b[0;32m      9\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m, in \u001b[0;36mprepare_dataset\u001b[1;34m(dataset, img_size)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pc \u001b[38;5;129;01min\u001b[39;00m point_clouds:\n\u001b[0;32m     11\u001b[0m     pc_images\u001b[38;5;241m.\u001b[39mappend(convert_pc_to_image(pc, img_size))\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpoint_clouds\u001b[38;5;241m.\u001b[39mindex(pc)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(point_clouds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m point clouds to images.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Store images in numpy array\u001b[39;00m\n\u001b[0;32m     14\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(pc_images)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1512,22) (1533,22) "
     ]
    }
   ],
   "source": [
    "labels = [\"2lanes\", \"3lanes\", \"crossing\", \"split4lanes\", \"split6lanes\", \"transition\"]\n",
    "# folder_path = \"C:/Users/jonas/OneDrive/Dokumente/Studium/Master/SoSe 2025/Machine Learning for Civil Engineering/Group Project/4_Road_point_cloud_classification/dataset/\"  # Adjust the folder path as needed\n",
    "dataset = load_point_clouds(labels)  # Load point clouds from the specified folder\n",
    "img_size = (400, 400)  # Define the size of the output images\n",
    "# Prepare the dataset for deep learning\n",
    "X, y = prepare_dataset(dataset, img_size=img_size)\n",
    "\n",
    "# Perform a train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "# Optionally visualize the first point cloud from each category as an image\n",
    "for i, label in enumerate(labels):\n",
    "    show_image(convert_pc_to_image(X_train[i]), title=f\"Point Cloud - {label} - {X_train[i].shape[0]} points\")\n",
    "\n",
    "# Build the CNN model\n",
    "input_shape = (3, img_size)  # Assuming RGB images of size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ce190f",
   "metadata": {},
   "source": [
    "# RandLANet\n",
    "Finally, after some literature research, we came up with the idea to classify our dataset by using an adapted version of the semantic segmentation model RandLANet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
